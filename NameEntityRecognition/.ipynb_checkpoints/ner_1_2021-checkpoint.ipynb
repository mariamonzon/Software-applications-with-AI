{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GgDUExqz5ioj"
   },
   "source": [
    "Set up Google Colab Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install google-colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3chPOl3o5MrV",
    "outputId": "5181c95e-994c-423c-ea06-1c778abed17a"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VXJqiu7P6T2p"
   },
   "source": [
    "Deep learning needs the computational power of GPUs, therefore, we run this notebook on Google Colab with GPU support.<br>\n",
    "Check RAM of GPU.<br>\n",
    "For this end, we need to install [GPUtil](https://pypi.org/project/GPUtil/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gumhOAG16vYh",
    "outputId": "b974f58f-31bd-4d1f-b9bf-8763bdcbbf65"
   },
   "outputs": [],
   "source": [
    "!pip install GPUtil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "WkK4Lfch6MGo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen RAM Free: 7.6 GB  I Proc size: 62.0 MB\n",
      "GPU RAM Free: 1984MB | Used: 64MB | Util   3% | Total 2048MB\n"
     ]
    }
   ],
   "source": [
    "import psutil\n",
    "import humanize\n",
    "import os\n",
    "import GPUtil as GPU\n",
    "GPUs = GPU.getGPUs()\n",
    "gpu = GPUs[0]\n",
    "\n",
    "def printm():\n",
    "  process = psutil.Process(os.getpid())\n",
    "  print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" I Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
    "  print('GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB'.format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
    "\n",
    "printm()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M3-quF-P-XXH"
   },
   "source": [
    "Change directory to \"/flair\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "BBTFZUFp_fY2",
    "outputId": "9c0f4341-10b3-49f5-d64a-5b7dc7e755eb"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-a78b1fb2bab9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lRq9VJeA_sSH"
   },
   "source": [
    "So, we are currently in \"/content\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_1VO3JH8ADUt",
    "outputId": "1b2fa206-d1a2-4749-d153-e36357139c1e"
   },
   "outputs": [],
   "source": [
    "for directory in os.walk( os.getcwd() ):\n",
    "\n",
    "  print( directory )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhCgEKfqA8mU"
   },
   "source": [
    "We see in the table above, that there exists a folder \"/content/gdrive/MyDrive/flair\" into which we have loaded our data.<br>\n",
    "We now change the working directory to this directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LefqzTtQA229"
   },
   "outputs": [],
   "source": [
    "os.chdir( \"/content/gdrive/MyDrive/flair\" ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "r88--TlKBW9r",
    "outputId": "f8ab1475-7f48-4071-e95d-c4fe7c097ddb"
   },
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the code from above, we change the current working directory to the directory, where we loaded our data to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1KpNb75WC1RS",
    "outputId": "7aaac04b-a7e8-4a62-92bf-33ccbe1794e7"
   },
   "outputs": [],
   "source": [
    "os.listdir( os.getcwd() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PvSVxTV4ERCb"
   },
   "source": [
    "Install Flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s6UgvIiOET3h",
    "outputId": "9ab5e29a-2300-4cd1-fadc-35aa1b69754c"
   },
   "outputs": [],
   "source": [
    "pip install flair"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OKbUtk3gUIUH"
   },
   "source": [
    "Import the Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0hwSlDozTvTu"
   },
   "outputs": [],
   "source": [
    "path_to_data = os.getcwd() + '/Entity Recognition in Resumes.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task 1__: Complete the following code: we want to read the data from said .json-file into the list __imported_data__. Also, print the first line, so that we get an idea, what the data look like. Also, print how many resumees we got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "StK5oNS8Uo7G",
    "outputId": "a0affbe4-fce5-4cba-9c7c-5cb49580c53c"
   },
   "outputs": [],
   "source": [
    "myfile = open( path_to_data, \"r\", encoding = \"utf-8\" )\n",
    "\n",
    "imported_data = []\n",
    "\n",
    "for datum in myfile:\n",
    "    \n",
    "    # TODO process data\n",
    "\n",
    "myfile.close()\n",
    "\n",
    "# TODO print first line\n",
    "\n",
    "# TODO print how many resumees were read in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I1ZSnIfOWLP_"
   },
   "source": [
    "Map the dataset to json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "We0edT5BWRTv"
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vI-fguXrWTvo"
   },
   "outputs": [],
   "source": [
    "mapped_data = [ json.loads( datum ) for datum in imported_data  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the data are stored in __mapped_data__. __mapped_data__ is a list, and its entries are dictionaries.<br>\n",
    "__Task 2__: choose an entry of __mapped_data__, iterate over its keys, print the key and its corresponding value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "exSHAxQFYBbp",
    "outputId": "e7ed7dbd-2c18-4023-d9c6-7e982868fccd"
   },
   "outputs": [],
   "source": [
    "#TODO choose an entry of mapped_data, iterate over its keys, print the key and its corresponding value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8gp7URulapTm"
   },
   "source": [
    "__Question 1__: What are the keys, and what information do these keys store?<br>\n",
    "__Task 3__: for each entry of __annotations__: iterate over the keys of this entry and print the key and the corresponding value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K9WcXpq4bTee",
    "outputId": "516847ab-74ff-4823-d920-925aaa6f92bb"
   },
   "outputs": [],
   "source": [
    "# TODO for each entry of annotations: iterate over the keys of this entry and print the key and the corresponding value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6i-ZPKiyfyrt"
   },
   "source": [
    "__Question 2__: What did you learn about the __annotations__? Give an example.<br>\n",
    "__Task 4__: Complete the following code to map the __mapped_data__ to a format, from which Spacy can learn. Print the first converted resumee for inspection. Choose one resumee. For this resumee, print all the data in __entities__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mzmu0XSOhybH",
    "outputId": "102ffd61-e51e-4d4b-fd73-0f1e14c448d5"
   },
   "outputs": [],
   "source": [
    "## data conversion method\n",
    "def convert_data(data):\n",
    "    \"\"\"\n",
    "    Creates NER training data in Spacy format from JSON dataset\n",
    "    Outputs the Spacy training data which can be used for Spacy training.\n",
    "    \"\"\"\n",
    "    text = data['content']\n",
    "    entities = []\n",
    "    if data['annotation'] is not None:\n",
    "        for annotation in data['annotation']:\n",
    "            # only a single point in text annotation.\n",
    "            point = annotation['points'][0]\n",
    "            labels = annotation['label']\n",
    "            # handle both list of labels or a single label.\n",
    "            if not isinstance(labels, list):\n",
    "                labels = [labels]\n",
    "            for label in labels:\n",
    "                # dataturks indices are both inclusive [start, end] but spacy is not [start, end)\n",
    "                entities.append((point['start'], point['end'] + 1, label))\n",
    "    return (text, {\"entities\": entities})\n",
    "   \n",
    "## TODO using a loop or list comprehension, convert each resume in mapped_data using the convert function above, \n",
    "## storing the result\n",
    "converted_resumes = None\n",
    "## TODO print the number of resumes in converted resumes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2jO6WUBLiaD0",
    "outputId": "0e7852c2-0e09-465c-e270-3f32c8f6b7ef"
   },
   "outputs": [],
   "source": [
    "# TODO print the first resumee for inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6FDv_kfXi4MF",
    "outputId": "2d477b09-cdf8-4fff-8816-cb499718bfc6"
   },
   "outputs": [],
   "source": [
    "# TODO choose one resumee. For this resumee, print all the data in entities. Use the dumps function from json."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lXx23PxAjPBj"
   },
   "source": [
    "__Task 5__: Explain the printout from above.<br>\n",
    "__Task 6__: Since some resumees have no annotation, we want to filter these out. You can recognize these resumees by them having no entries in __entities__. Pick one of the remaining resumees, iterate over the items in __entitities__, print for each item the label and the corresponding text from the resumee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8KjAdqTckp41"
   },
   "outputs": [],
   "source": [
    "# TODO filter out the resumees whose entities have no entries.\n",
    "converted_complete_resumees = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Al0x9iyBk-qs",
    "outputId": "67322422-b620-4e67-a946-3a34a3b314e3"
   },
   "outputs": [],
   "source": [
    "print( len( converted_complete_resumees ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qys_4RJ6nCCj",
    "outputId": "47be654b-c18c-44cb-fe70-3fa58d4f86c3"
   },
   "outputs": [],
   "source": [
    "# TODO Now, pick one resumee from converted_complete_resumees, and iterate over the items in \n",
    "# entities. Print for each item the label and the corresponding text from the resumee."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QJCOSsZJtDnA"
   },
   "source": [
    "__Question 3__: Are the labels unique? If the labels are not unique, does this make the data ambigious?<br>\n",
    "__Task 7__: Collect the names of all entities in __converted_complete_resumee__ dataset, and store these names in __all_labels__. Then iterate over the contents of __all_labels__, and store each name that does not appear in __unique_labels__ in __unique_labels__, so that __unique_labels__ contains each name that appears in __all_labels__, but only once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i8N5z6OutflX",
    "outputId": "76fa67e2-724e-44f4-c640-b81164dd89aa"
   },
   "outputs": [],
   "source": [
    "# TODO Collect the names of all entities in converted_complete_resumee dataset, and store these names \n",
    "# in all_labels. Then iterate over the contents of all_labels, and store each name that does not appear \n",
    "# in unique_labels in unique_labels, so that unique_labels contains each name that appears in \n",
    "# all_labels, but only once.\n",
    "all_labels = []\n",
    "\n",
    "unique_labels = []\n",
    "\n",
    "for res in converted_complete_resumees:\n",
    "\n",
    "for item in unique_labels:\n",
    "\n",
    "  print( item )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fxJ_ZC6Zv0q1"
   },
   "source": [
    "__Task 8__: Now choose three labels, on which you want to train your named entity recognition algorithm. You want to have for each label at least 300 documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3KgZMpQTwiIu",
    "outputId": "a4e68553-77d1-430c-d2d7-0b878129af5d"
   },
   "outputs": [],
   "source": [
    "## TODO store entity label names for the entities you want to work with in an array \n",
    "chosen_entity_label = [ None, None, None ]\n",
    "## for each chosen entity label, count how many documents have a labeled entity for that label, and how many labeled entities total there are \n",
    "## for that entity\n",
    "for chosen in chosen_entity_label:\n",
    "    found_docs_with_entity = 0\n",
    "    entity_count = 0\n",
    "    for resume in converted_complete_resumees:\n",
    "        entity_list = resume[1][\"entities\"]\n",
    "        _,_,labels = zip(*entity_list)\n",
    "        if chosen in labels:\n",
    "            found_docs_with_entity+=1\n",
    "            entity_count+=len([l for l in labels if l == chosen])\n",
    "    print(\"Docs with {}: {}\".format(chosen,found_docs_with_entity))\n",
    "    print(\"Total count of {}: {}\".format(chosen,entity_count))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "resumee_ner.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
