{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "ner_2_2021.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7yjL03CKOoEk"
   },
   "source": [
    "Getting started with Spacy<br>\n",
    "Import data.<br>\n",
    "We repeat the preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vOxAAu3cOm9D",
    "outputId": "26bb0fd0-bc22-492d-defa-c8fb98fa9613"
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HOc5LrSCPSEC"
   },
   "source": [
    "import os"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jd_X8KyOPWlJ"
   },
   "source": [
    "os.chdir( \"/content/gdrive/MyDrive/flair\" ) "
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NBNxx2f8Pcmq"
   },
   "source": [
    "path_to_data = os.getcwd() + '/Entity Recognition in Resumes.json'"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dUKwMlfkaR2n",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "51befe5a-f848-4ee1-e9f9-c08638c26545"
   },
   "source": [
    "myfile = open( path_to_data, \"r\", encoding = \"utf-8\" )\n",
    "\n",
    "imported_data = []\n",
    "\n",
    "for datum in myfile:\n",
    "    \n",
    "  # process data as list\n",
    "  imported_data.append(datum)\n",
    "\n",
    "myfile.close()\n",
    "\n",
    "# print first line\n",
    "print(\"First Line:\\t\", imported_data[0])\n",
    "\n",
    "# print how many resumees were read in\n",
    "print(\"Total Number of Resumes:\\t\", len(imported_data))\n"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "First Line:\t {\"content\": \"Afreen Jamadar\\nActive member of IIIT Committee in Third year\\n\\nSangli, Maharashtra - Email me on Indeed: indeed.com/r/Afreen-Jamadar/8baf379b705e37c6\\n\\nI wish to use my knowledge, skills and conceptual understanding to create excellent team\\nenvironments and work consistently achieving organization objectives believes in taking initiative\\nand work to excellence in my work.\\n\\nWORK EXPERIENCE\\n\\nActive member of IIIT Committee in Third year\\n\\nCisco Networking -  Kanpur, Uttar Pradesh\\n\\norganized by Techkriti IIT Kanpur and Azure Skynet.\\nPERSONALLITY TRAITS:\\n• Quick learning ability\\n• hard working\\n\\nEDUCATION\\n\\nPG-DAC\\n\\nCDAC ACTS\\n\\n2017\\n\\nBachelor of Engg in Information Technology\\n\\nShivaji University Kolhapur -  Kolhapur, Maharashtra\\n\\n2016\\n\\nSKILLS\\n\\nDatabase (Less than 1 year), HTML (Less than 1 year), Linux. (Less than 1 year), MICROSOFT\\nACCESS (Less than 1 year), MICROSOFT WINDOWS (Less than 1 year)\\n\\nADDITIONAL INFORMATION\\n\\nTECHNICAL SKILLS:\\n\\n• Programming Languages: C, C++, Java, .net, php.\\n• Web Designing: HTML, XML\\n• Operating Systems: Windows […] Windows Server 2003, Linux.\\n• Database: MS Access, MS SQL Server 2008, Oracle 10g, MySql.\\n\\nhttps://www.indeed.com/r/Afreen-Jamadar/8baf379b705e37c6?isid=rex-download&ikw=download-top&co=IN\",\"annotation\":[{\"label\":[\"Email Address\"],\"points\":[{\"start\":1155,\"end\":1198,\"text\":\"indeed.com/r/Afreen-Jamadar/8baf379b705e37c6\"}]},{\"label\":[\"Links\"],\"points\":[{\"start\":1143,\"end\":1239,\"text\":\"https://www.indeed.com/r/Afreen-Jamadar/8baf379b705e37c6?isid=rex-download&ikw=download-top&co=IN\"}]},{\"label\":[\"Skills\"],\"points\":[{\"start\":743,\"end\":1140,\"text\":\"Database (Less than 1 year), HTML (Less than 1 year), Linux. (Less than 1 year), MICROSOFT\\nACCESS (Less than 1 year), MICROSOFT WINDOWS (Less than 1 year)\\n\\nADDITIONAL INFORMATION\\n\\nTECHNICAL SKILLS:\\n\\n• Programming Languages: C, C++, Java, .net, php.\\n• Web Designing: HTML, XML\\n• Operating Systems: Windows […] Windows Server 2003, Linux.\\n• Database: MS Access, MS SQL Server 2008, Oracle 10g, MySql.\"}]},{\"label\":[\"Graduation Year\"],\"points\":[{\"start\":729,\"end\":732,\"text\":\"2016\"}]},{\"label\":[\"College Name\"],\"points\":[{\"start\":675,\"end\":702,\"text\":\"Shivaji University Kolhapur \"}]},{\"label\":[\"Degree\"],\"points\":[{\"start\":631,\"end\":672,\"text\":\"Bachelor of Engg in Information Technology\"}]},{\"label\":[\"Graduation Year\"],\"points\":[{\"start\":625,\"end\":629,\"text\":\"2017\\n\"}]},{\"label\":[\"College Name\"],\"points\":[{\"start\":614,\"end\":622,\"text\":\"CDAC ACTS\"}]},{\"label\":[\"Degree\"],\"points\":[{\"start\":606,\"end\":611,\"text\":\"PG-DAC\"}]},{\"label\":[\"Companies worked at\"],\"points\":[{\"start\":438,\"end\":453,\"text\":\"Cisco Networking\"}]},{\"label\":[\"Email Address\"],\"points\":[{\"start\":104,\"end\":147,\"text\":\"indeed.com/r/Afreen-Jamadar/8baf379b705e37c6\"}]},{\"label\":[\"Location\"],\"points\":[{\"start\":62,\"end\":67,\"text\":\"Sangli\"}]},{\"label\":[\"Name\"],\"points\":[{\"start\":0,\"end\":13,\"text\":\"Afreen Jamadar\"}]}],\"extras\":null,\"metadata\":{\"first_done_at\":1527844872000,\"last_updated_at\":1537724086000,\"sec_taken\":0,\"last_updated_by\":\"BIQNZm4INNfvByMqkaVwVt6OZTv2\",\"status\":\"done\",\"evaluation\":\"CORRECT\"}}\n",
      "\n",
      "Total Number of Resumes:\t 701\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SxQojbZyPrkh"
   },
   "source": [
    "import json"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Axwa389baXDN"
   },
   "source": [
    "mapped_data = [ json.loads( datum ) for datum in imported_data  ]"
   ],
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d2FaAX-xabje",
    "outputId": "7f809ef9-a033-4f9e-93ee-d8d77a352726"
   },
   "source": [
    "## data conversion method\n",
    "def convert_data(data):\n",
    "    \"\"\"\n",
    "    Creates NER training data in Spacy format from JSON dataset\n",
    "    Outputs the Spacy training data which can be used for Spacy training.\n",
    "    \"\"\"\n",
    "    text = data['content']\n",
    "    entities = []\n",
    "    if data['annotation'] is not None:\n",
    "        for annotation in data['annotation']:\n",
    "            # only a single point in text annotation.\n",
    "            point = annotation['points'][0]\n",
    "            labels = annotation['label']\n",
    "            # handle both list of labels or a single label.\n",
    "            if not isinstance(labels, list):\n",
    "                labels = [labels]\n",
    "            for label in labels:\n",
    "                # dataturks indices are both inclusive [start, end] but spacy is not [start, end)\n",
    "                entities.append((point['start'], point['end'] + 1, label))\n",
    "    return (text, {\"entities\": entities})\n",
    "   \n",
    "## Using a loop or list comprehension, convert each resume in mapped_data using the convert function above, \n",
    "## storing the result\n",
    "converted_resumes = [convert_data(resume) for resume in mapped_data]\n",
    "## print the number of resumes in converted resumes \n",
    "print(\"Converted resumes:\\t\", len(converted_resumes))"
   ],
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Converted resumes:\t 701\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GapDFuttaqpk"
   },
   "source": [
    "# filter out the resumees whose entities have no entries.\n",
    "converted_complete_resumees = [resume for resume in converted_resumes if len(resume[1][\"entities\"]) > 0]"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1-UWPUvfbo0l"
   },
   "source": [
    "Up until now, you could reuse the code from the previous notebook, now, something new comes."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CYhNshfBbl8d",
    "outputId": "7cedc694-f88b-48d7-de65-c00782fa4f7f"
   },
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "print(nlp)"
   ],
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "<spacy.lang.en.English object at 0x7f4b5f439510>\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9AqzhVdHb-xO"
   },
   "source": [
    "__nlp__ is Spacy's English language model. For this model, a pretrained NER-model exists."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TDHFtX6Lb-CY",
    "outputId": "e292f413-11e4-46a9-b2d7-3f09849d2945"
   },
   "source": [
    "ner = nlp.get_pipe('ner')\n",
    "labels = ner.labels\n",
    "print(labels)"
   ],
   "execution_count": 11,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "('CARDINAL', 'DATE', 'EVENT', 'FAC', 'GPE', 'LANGUAGE', 'LAW', 'LOC', 'MONEY', 'NORP', 'ORDINAL', 'ORG', 'PERCENT', 'PERSON', 'PRODUCT', 'QUANTITY', 'TIME', 'WORK_OF_ART')\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sBy9Uo20cZH2"
   },
   "source": [
    "__Question 1__: Explain the labels __GPE__, __FAC__, __NORP__.<br>\n",
    "\n",
    "The labbels refer to Countries, cities, states (GPE) , buildings( FAC) and Nationalities or Social Groups (NORP)\n",
    "\n",
    "Which of these labels from __ner__ do you think will Spacy recognize in the resumees?<br>\n",
    "__Task 1__: choose a resumee."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SmYFT_EKzQfg",
    "outputId": "2ec0a628-b00f-457b-9afd-4620eb97c4bb"
   },
   "source": [
    "\n",
    "for label in ['GPE', 'FAC', 'NORP']:\n",
    "    print(label, \":\\t\", spacy.explain(label))"
   ],
   "execution_count": 12,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "GPE :\t Countries, cities, states\n",
      "FAC :\t Buildings, airports, highways, bridges, etc.\n",
      "NORP :\t Nationalities or religious or political groups\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4yXayIFffE7F",
    "outputId": "da872f3c-81a1-4736-9641-415d160b8179"
   },
   "source": [
    "# get a single resume text and print it out.\n",
    "resume = 24\n",
    "restxt = converted_complete_resumees[resume][0]\n",
    "## print it out, removing extraneous spaces\n",
    "print(\"\\n\".join(restxt.split('\\n\\n')))"
   ],
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Karthihayini C\n",
      "Systems Engineer - Infosys Limited\n",
      "Rajapalaiyam, Tamil Nadu - Email me on Indeed: indeed.com/r/Karthihayini-\n",
      "C/627254c443836b3c\n",
      "To be a part of challenging team, which works for the growth of an organization, explores my\n",
      "potential and provides me an opportunity to enhance my knowledge and to be an asset of the\n",
      "company.\n",
      "Willing to relocate: Anywhere\n",
      "WORK EXPERIENCE\n",
      "Systems Engineer\n",
      "Infosys Limited -  Chennai, Tamil Nadu -\n",
      "December 2016 to Present\n",
      "Trainee\n",
      "Infosys Limited -  Mysore, Karnataka -\n",
      "June 2016 to November 2016\n",
      "Client: Renault\n",
      "Project Details:\n",
      "• Trained in .Net technology.\n",
      "• Handled VB part of the application which has mainframe as backend and Visual basic as the\n",
      "front end with annuaire service as the medium between mainframe and VB.\n",
      "• Took care of site creation in the Share Point technology.\n",
      "• Experienced in using SmartSVN, TortoiseSVN, DB Visualiser.\n",
      "EDUCATION\n",
      "B E in Production Engineering\n",
      "Velammal Engineering College -  Chennai, Tamil Nadu\n",
      "2016\n",
      "Tamil Nadu Board Of Education -  Rajapalaiyam, Tamil Nadu\n",
      "2012\n",
      "Education\n",
      "Hr. Sec. School\n",
      "2010\n",
      "https://www.indeed.com/r/Karthihayini-C/627254c443836b3c?isid=rex-download&ikw=download-top&co=IN\n",
      "https://www.indeed.com/r/Karthihayini-C/627254c443836b3c?isid=rex-download&ikw=download-top&co=IN\n",
      "\n",
      "SKILLS\n",
      "Destructive Testing (Less than 1 year), FORGE (Less than 1 year), Non-Destructive (Less than 1\n",
      "year), Non-Destructive Testing (Less than 1 year), quality control (Less than 1 year)\n",
      "ADDITIONAL INFORMATION\n",
      "AREAS OF INTEREST\n",
      "1. Strength of materials\n",
      "2. Engineering statistics and quality control\n",
      "INDUSTRIAL VISITS\n",
      "• BAY FORGE, Madurantakam\n",
      "• ASHOK LEYLAN, Ennore\n",
      "• CARBORUNDUM, Thiruvottiyur\n",
      "• BONFIGLIOLI, Thirumudivakkam\n",
      "IN-PLANT TRAININGS & WORKSHOPS\n",
      "• Attended in-plant training on security division at \"Minda Corporation Limited\", Chennai.\n",
      "• Attended in-plant training in \"BHEL\", Ranipet.\n",
      "• Attended workshop on \"Aero Modeling\" conducted at Consto UAV Technologies\n",
      "• Attended workshop on \"Non-Destructive Testing\" conducted at Velammal engineering College,\n",
      "Chennai.\n",
      "PERSONAL QUALIFICATIONS:\n",
      "• Ability to quick grasp the concepts\n",
      "• Flexible\n",
      "• Hard working\n",
      "• Consistent\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VeggUYGfuyS9"
   },
   "source": [
    "Next, we let __nlp__ process that single resumee.<br>\n",
    "__Task 2__: print the results in __doc__. For each result, print the underlying text and the label."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cp961WAxfW8N",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "8d95db44-5e79-4491-bab4-c6c0febc7021"
   },
   "source": [
    "doc = nlp(restxt)\n",
    "\n",
    "# Print the results in doc. For each result, print the text and the label.\n",
    "print([ (token.text, token.label)  for token in doc.ents])"
   ],
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "[('Karthihayini C', 383), ('Rajapalaiyam', 384), ('Tamil Nadu - Email', 380), ('Infosys Limited -  Chennai', 383), ('Tamil Nadu -', 380), ('December 2016', 391), ('Trainee', 380), ('Mysore', 384), ('Karnataka', 384), ('June 2016 to November 2016', 391), ('Renault', 383), ('Handled', 386), ('VB', 386), ('Share Point', 383), ('DB Visualiser', 380), ('Production Engineering\\n\\n', 383), ('Tamil Nadu', 380), ('2016', 391), ('Tamil Nadu Board Of Education', 383), ('Rajapalaiyam', 380), ('Tamil Nadu', 380), ('2012', 391), ('2010', 391), ('SKILLS', 383), ('Destructive Testing', 380), ('Less than 1 year', 391), ('Less than 1 year', 391), ('Non-Destructive', 383), ('Non-Destructive Testing', 383), ('Less than 1 year', 391), ('Less than 1 year', 391), ('1', 397), ('2', 397), ('Madurantakam', 380), ('ASHOK', 383), ('CARBORUNDUM', 383), ('Thiruvottiyur', 381), ('Thirumudivakkam', 380), ('IN-PLANT TRAININGS & WORKSHOPS', 383), ('Minda Corporation Limited', 383), ('Chennai', 389), ('BHEL', 388), ('Ranipet', 384), ('Aero Modeling', 388), ('Consto UAV Technologies', 383), ('Non-Destructive Testing', 388), ('Velammal engineering College', 383)]\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xok9PDMplMNe"
   },
   "source": [
    "__Question 2__: How well did Spacy perform at recognizing the labels for this text?<br>\n",
    "\n",
    "When Spacy predicted the labels for this resumee, a pretrained model was used.\n",
    "<br>\n",
    "__Task 3__: print for this resumee the original labels and their corresponding text."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V8eIm9lXlznU",
    "outputId": "e23c28de-15ad-48b9-808b-205672aad831"
   },
   "source": [
    "# TODO print for that resumee the original labels and their corresponding text.\n",
    "labeled_ents = converted_complete_resumees[resume][1][\"entities\"]\n",
    "for ent in labeled_ents: \n",
    "  print(ent[-1],\":\\t\", restxt[ent[0]:ent[1]] )"
   ],
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Skills :\t Consistent\n",
      "Skills :\t Hard working\n",
      "Skills :\t  Flexible\n",
      "Skills :\t Ability to quick grasp the concepts\n",
      "Skills :\t Destructive Testing (Less than 1 year), FORGE (Less than 1 year), Non-Destructive (Less than 1\n",
      "year), Non-Destructive Testing (Less than 1 year), quality control (Less than 1 year)\n",
      "\n",
      "College Name :\t Hr. Sec. School\n",
      "\n",
      "Location :\t Rajapalaiyam\n",
      "College Name :\t Tamil Nadu Board Of Education\n",
      "College Name :\t Velammal Engineering College\n",
      "Degree :\t B E in Production Engineering\n",
      "Companies worked at :\t Infosys Limited\n",
      "Companies worked at :\t Infosys Limited\n",
      "Designation :\t Systems Engineer\n",
      "Email Address :\t indeed.com/r/Karthihayini-\n",
      "C/627254c443836b3c\n",
      "Location :\t Rajapalaiyam\n",
      "Companies worked at :\t Infosys Limited\n",
      "\n",
      "Companies worked at :\t Infosys Limited\n",
      "Designation :\t Systems Engineer\n",
      "Name :\t Karthihayini C\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aBHplvL2KHXO"
   },
   "source": [
    "__Question 3__: Compare the performance of the pretrained model __nlp__ and the true labels. Did Spacy perform well? If not, try to explain why.<br>\n",
    "\n",
    "It did not perform well \n",
    "\n",
    "__Task 4__: You chose three labels. Select all the resumees, in which all three labels appear."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wELEGAoEKDwf",
    "outputId": "4662c43c-d90b-4409-827f-b0d3ccfb821f"
   },
   "source": [
    "# fill in your chosen labels in chosen_entity_labels\n",
    "chosen_entity_labels = [ \"Skills\", \"College Name\", \"Email Address\" ]\n",
    "\n",
    "## this method gathers all resumes which have all of the chosen entites above.\n",
    "def gather_candidates(dataset,entity_labels):\n",
    "    candidates = list()\n",
    "    for resume in dataset:\n",
    "        res_ent_labels = list(zip(*resume[1][\"entities\"]))[2]\n",
    "        if set(entity_labels).issubset(res_ent_labels):\n",
    "            candidates.append(resume)\n",
    "    return candidates\n",
    "\n",
    "training_data = gather_candidates( converted_complete_resumees, chosen_entity_labels )\n",
    "print(\"Gathered c training examples\".format(len(training_data)))"
   ],
   "execution_count": 20,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Gathered c training examples\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xSDI7dDDLBbW"
   },
   "source": [
    "__Task 5__: Next, we want to remove all other entities, since we only want to train NER for the three entities in __chosen_entity_labels__."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1DBr_VzxLNqe"
   },
   "source": [
    "## filter all annotation based on filter list\n",
    "def filter_ents(ents, filter):\n",
    "    filtered = [ent for ent in ents if ent[2] in filter]\n",
    "    return filtered\n",
    "\n",
    "## use method above to remove all but relevant (chosen) entity annotations and store in X variable. X shall contain all\n",
    "## the resumees from training_data, but their entity annotations shall be filtered using the function from above.\n",
    "X = [(sample[0], {\"entities\": filter_ents(sample[1][\"entities\"], chosen_entity_labels)}) for sample in training_data]\n"
   ],
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZpyxtX6NteV"
   },
   "source": [
    "__Task 6__: Some resumees cause trouble. We filter these out with the following lines of code.<br>\n",
    "First, use __add_label__ to add your chosen labels to the __ner__ model."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RqwcgW_xX18e",
    "outputId": "c32de0d5-8702-42e9-addd-64dacf993a35"
   },
   "source": [
    "from spacy.gold import GoldParse \n",
    "\n",
    "\n",
    "print(\"Selected Labels: \", chosen_entity_labels )\n",
    "# add your labels \n",
    "\n",
    "ner.add_label(\"Skills\")\n",
    "\n",
    "ner.add_label(\"College Name\")\n",
    "\n",
    "ner.add_label(\"Email Address\")\n",
    "\n",
    "nlp.begin_training()\n",
    "\n",
    "good = []\n",
    "\n",
    "for item in X:\n",
    "  \n",
    "  text = nlp.make_doc( item[ 0 ] )\n",
    "\n",
    "  try:\n",
    "    \n",
    "    gold = GoldParse( text, entities = item[ 1 ][ \"entities\" ] )\n",
    "\n",
    "  except ValueError:\n",
    "\n",
    "    continue\n",
    "  \n",
    "  try:\n",
    "    \n",
    "    nlp.update( [ text ], [ gold ], drop = 0.3 )\n",
    "\n",
    "  except Exception:\n",
    "\n",
    "    pass\n",
    "\n",
    "  else:\n",
    "\n",
    "    good.append( item )\n",
    "\n",
    "print( \"Number of good samples: \" + str( len( good ) ) )\n",
    "\n",
    "print( \"\" )\n",
    "\n",
    "print( \"\" )\n",
    "\n",
    "print( \"Number of bad sampples: \" + str( len( X ) - len( good ) ) )"
   ],
   "execution_count": 22,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Selected Labels:  ['Skills', 'College Name', 'Email Address']\n",
      "Number of good samples: 339\n",
      "\n",
      "\n",
      "Number of bad sampples: 36\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "obZ93SLzyFIe"
   },
   "source": [
    "For a machine learning model, it is essential to be able to generalize. Only a model, that can generalize well is able to process new data in a meaningful way. Therefore, one usually separates the data set into two sets: the training set and the test set. The training set is used to train the model. The test set is used to evaluate the performance of the model on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "k0XVZPTyyh4T"
   },
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split( [ item[ 0 ] for item in good ], [ item[ 1 ] for item in good ], test_size = 0.3 )"
   ],
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3AcgIty-1otR"
   },
   "source": [
    "__Task 7__: Complete the following code. Shuffle __new_index__. Create the data sets __x_shuffled__ and __y_shuffled__. Use these to create minibatches, iterate over these minibatches, preprocess the data in a given minibatch using __nlp.make_doc__ and __GoldParse__. Employ __nlp.update__ to update the model using these preprocessed data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zGLUALLr1rEp"
   },
   "source": [
    "import numpy as np\n",
    "from spacy.util import minibatch, compounding\n",
    "\n",
    "nlp.begin_training()\n",
    "\n",
    "new_index = np.arange( len( x_train ) )\n",
    "\n",
    "x_data = np.array( x_train )\n",
    "\n",
    "y_data = np.array( y_train )\n",
    "\n",
    "for i in range( 20 ):\n",
    "\n",
    "  # TODO shuffle new_index\n",
    "  np.random.shuffle(new_index)\n",
    "\n",
    "  x_shuffled = x_data[new_index]  #TODO create x_shuffled from x_data by using the shuffled new_index\n",
    "\n",
    "  y_shuffled = y_data[new_index]  #TODO create y_shuffled from y_data by using the shuffled new_index\n",
    "\n",
    "\n",
    "\n",
    "  # divide the data in x_shuffled and y_shuffled into minibatches of identical size\n",
    "  n_batch = 3\n",
    "  batch_size = int(len(x_shuffled) / n_batch)\n",
    "  minibatches =[]\n",
    "  # iterate over these minibatches\n",
    "  for b in range(n_batch):\n",
    "    x_batch = x_shuffled[b*batch_size: (b+1) * batch_size]\n",
    "    y_batch = y_shuffled[b*batch_size: (b+1) * batch_size]\n",
    "\n",
    "    # preprocess the data in a minibatch using nlp.make_doc and GoldParse\n",
    "    #docs = nlp.make_doc(str(x_batch))\n",
    "    docs =[None]*len(x_batch)\n",
    "    golds=[None]*len(y_batch)\n",
    "    for i, (text_sample, label) in enumerate(zip(x_batch, y_batch)):\n",
    "      docs[i] = nlp.make_doc(str(text_sample))\n",
    "      golds[i]=  GoldParse(docs[i], entities=label[\"entities\"] )\n",
    "  # use these preprocessed data and nlp.update to train the model\n",
    "  nlp.update( docs, golds, drop = 0.3 )"
   ],
   "execution_count": 24,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQSe2-BOYlWN"
   },
   "source": [
    "__Question 4__: Why did we shuffle the data?<br>\n",
    "\n",
    "We shuffle the data in order to improve results and avoid to memorize the order. \n",
    "\n",
    "Why did we employ mini batches?<br>\n",
    "For better computation efficiency and optimization\n",
    "\n",
    "Reasearch the term __epoch__ in machine learning. How many epochs of training do we employ?<br>\n",
    "__Task 8__: Next, we choose one resumee and print it."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SFGKoHVXY1pL",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "51bd0058-648e-443f-c641-54edf6ac4078"
   },
   "source": [
    "n_resume = 3\n",
    "resume = str(x_test[n_resume])\n",
    "\n",
    "print( resume )"
   ],
   "execution_count": 29,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Ananya Chavan\n",
      "lecturer - oracle tutorials\n",
      "\n",
      "Mumbai, Maharashtra - Email me on Indeed: indeed.com/r/Ananya-\n",
      "Chavan/738779ab71971a96\n",
      "\n",
      "Seeking a responsible job with an opportunity for professional challenges and utilize my skills\n",
      "up to its extreme.\n",
      "\n",
      "WORK EXPERIENCE\n",
      "\n",
      "lecturer\n",
      "\n",
      "Oracle tutorials -  Mumbai, Maharashtra -\n",
      "\n",
      "April 2016 to Present\n",
      "\n",
      "for computer science (STD 11th and 12th) (2 years)\n",
      "➢ Worked at \"Dr.Babasaheb Ambedkar College, Chembur (W) \" as a lecturer for • B.Sc. (Computer\n",
      "Science & Information Technology)\n",
      "• F.Y.J.C. (Computer Science & I.T.)\n",
      "• S.Y.J.C. (Computer Science & I.T.)\n",
      "➢ Worked at \"LIVE\" as a Head of the IT Department and Lecturer for Web designing.\n",
      "➢ Worked at \"Kohinoor College Of Hotel Management\" as visiting lecturer for SEM I.\n",
      "➢ Working at \"ORACLE TUTORIALS\" as a lecturer for computer science (STD 11th and 12th)\n",
      "\n",
      "EDUCATION\n",
      "\n",
      "MCA\n",
      "\n",
      "Mumbai University -  Mumbai, Maharashtra\n",
      "\n",
      "B.Sc. in Com.Sci\n",
      "\n",
      "Mumbai University -  Mumbai, Maharashtra\n",
      "\n",
      "SKILLS\n",
      "\n",
      "SEARCH ENGINE MARKETING (2 years), SEM (2 years), ACCESS (Less than 1 year), AJAX (Less\n",
      "than 1 year), APACHE (Less than 1 year)\n",
      "\n",
      "ADDITIONAL INFORMATION\n",
      "\n",
      "Technical skills:\n",
      "Languages: C, C++, Java (J2EE),\n",
      "Web Component APIS:: Jdbc, Servlet, JSP.\n",
      "Frameworks: Spring 4 & Struts 2\n",
      "ORM Framework: Hibernate\n",
      "Web Development: Html5, CSS3, Java Script, Ajax &JQuery, Angular Js\n",
      "\n",
      "https://www.indeed.com/r/Ananya-Chavan/738779ab71971a96?isid=rex-download&ikw=download-top&co=IN\n",
      "https://www.indeed.com/r/Ananya-Chavan/738779ab71971a96?isid=rex-download&ikw=download-top&co=IN\n",
      "\n",
      "\n",
      "Application Servers: Apache Tomcat,\n",
      "IDE: Eclipse, Netbeans\n",
      "Database: Ms-Access, Mysql\n",
      "Operating Systems: Windows 7, 8, 10\n",
      "FTP Client: Filezilla\n",
      "Versioning Tools: Git\n",
      "\n",
      "Project Details:\n",
      "\n",
      "\"Real Estate Application\" (Client: Global Realtor PVT. LTD Pune)\n",
      "Front-End: Java (J2EE), JDBC, Servlet, JSP, Jquery.\n",
      "Back end: Mysql.\n",
      "Duration: 6 Month (Internship)\n",
      "Company Name: AryanTech India Pvt. Ltd. Pune\n",
      "My Role: Developer as Trainee.\n",
      "Module: Module 4.\n",
      "Description: Developed as a MCA Final SEM Project for\n",
      "\"Global Realtors PVT.LTD, Hinjewadi, Pune.\"\n",
      "The Real Estate Web Application is an interactive, effective and revenue-generating website\n",
      "designed for the Real Estate Industry. The main objective of this application is to help the Real\n",
      "Estate Company to display unlimited number of property listings on the website.\n",
      "\n",
      "\"Beauty Parlor Management System\" (B.Sc. (Com.Sci.))\n",
      "Tool: VB 6.0\n",
      "Language: VB\n",
      "Database: MS-Access\n",
      "Operating System: Windows XP\n",
      "The Beauty Parlor Management System is an easy and effective system to use. The main features\n",
      "of this system are to avoid manual work and keep storing all appointments of customers.\n",
      "\n",
      "\"Web Designing Project (Reptiles.com) \" (B.Sc. (Com.Sci.))\n",
      "Language: HTML and ASP\n",
      "Tool: Dreamweaver 8.0\n",
      "Database: MS-Access\n",
      "Operating System: Windows XP\n",
      "The Reptiles.com is a simple informative site. The main features of this system are to give all\n",
      "information of Snakes.\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ockGXX2nuyTI"
   },
   "source": [
    "__Task 9__: we process this resumee using __nlp__. Print for all items in __doc.ents__ the predicted label and the corresponding text. Then print the correct labels and their corresponding text for that resumee with data from __y_test__."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "arAyrdyfbnQR",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "aeb31005-2ed5-44cd-be6a-137e77642da5"
   },
   "source": [
    "doc = nlp( resume )\n",
    "\n",
    "# print for all the items in doc.ents the predicted label and the corresponding text\n",
    "print(\" Predicted  Labels\")\n",
    "for item in doc.ents:\n",
    "  for entry in item:\n",
    "    print(entry)\n",
    "\n",
    "print(\" Correct Labels\")\n",
    "# print the correct labels and their corresponding text for that resumee with data from y_test\n",
    "correct_ents = y_test[n_resume]['entities']\n",
    "for ent in correct_ents:\n",
    "    print(\"{}: {}\".format(ent[2], resume[ent[0]: ent[1]]))"
   ],
   "execution_count": 30,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      " Predicted  Labels\n",
      " Correct Labels\n",
      "Skills: EARCH ENGINE MARKETING (2 years), SEM (2 years), ACCESS (Less than 1 year), AJAX (Less\n",
      "than 1 year), APACHE (Less than 1 year)\n",
      "\n",
      "ADDITIONAL INFORMATION\n",
      "\n",
      "Technical skills:\n",
      "Languages: C, C++, Java (J2EE),\n",
      "Web Component APIS:: Jdbc, Servlet, JSP.\n",
      "Frameworks: Spring 4 & Struts 2\n",
      "ORM Framework: Hibernate\n",
      "Web Development: Html5, CSS3, Java Script, Ajax &JQuery, Angular Js\n",
      "\n",
      "https://www.indeed.com/r/Ananya-Chavan/738779ab71971a96?isid=rex-download&ikw=download-top&co=IN\n",
      "https://www.indeed.com/r/Ananya-Chavan/738779ab71971a96?isid=rex-download&ikw=download-top&co=IN\n",
      "\n",
      "\n",
      "Application Servers: Apache Tomcat,\n",
      "IDE: Eclipse, Netbeans\n",
      "Database: Ms-Access, Mysql\n",
      "Operating Systems: Windows 7, 8, 10\n",
      "FTP Client: Filezilla\n",
      "Versioning Tools: Git\n",
      "College Name: Mumbai University\n",
      "College Name: Mumbai University\n",
      "Email Address: indeed.com/r/Ananya-\n",
      "Chavan/738779ab71971a96\n",
      "\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DTjZw4RVeB87"
   },
   "source": [
    "__Question 5__: What labels did the model predict correctly?<br> \n",
    "Where appeared problems?<br> \n",
    "How can you explain the problems?<br>\n",
    "__Question 6__: We can evaluate the performance of the model using 4 metrics: the __Accuracy__, the __Precision__, the __Recall__ and __F1__.<br>\n",
    "Inform yourself on these metrics. How are they defined? Explain the concept of __True Positive__, __True Negative__, __False Positive__ and __False Negative__. Use these to define  the __Accuracy__, the __Precision__, the __Recall__ and __F1__, and also give the formula for each of these.<br>\n",
    "__Task 10__: Complete the following code. Call __make_bilou_df__ with a resume from the test set and store result in __bilou_df__ variable."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "W5M-ArsDtzFd",
    "scrolled": true,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "outputId": "2453b496-0ca9-44c7-913a-878c3ae1f86f"
   },
   "source": [
    "from spacy.gold import biluo_tags_from_offsets\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "## returns a pandas dataframe with tokens, prediction, and true (Gold Standard) annotations of tokens\n",
    "def make_bilou_df(nlp,resume):\n",
    "    \"\"\"\n",
    "    param nlp - a trained spacy model\n",
    "    param resume - a resume from our train or test set\n",
    "    \"\"\"\n",
    "    doc = nlp(resume[0])\n",
    "    bilou_ents_predicted = biluo_tags_from_offsets(doc, [(ent.start_char,ent.end_char,ent.label_)for ent in doc.ents])\n",
    "    bilou_ents_true = biluo_tags_from_offsets(doc, [(ent[0], ent[1], ent[2]) for ent in resume[1][\"entities\"]])\n",
    "\n",
    "    \n",
    "    doc_tokens = [tok.text for tok in doc]\n",
    "    bilou_df = pd.DataFrame()\n",
    "    bilou_df[\"Tokens\"] =doc_tokens\n",
    "    bilou_df[\"Tokens\"] = bilou_df[\"Tokens\"].str.replace(\"\\\\s+\",\"\") \n",
    "    bilou_df[\"Predicted\"] = bilou_ents_predicted\n",
    "    bilou_df[\"True\"] = bilou_ents_true\n",
    "    return bilou_df\n",
    "\n",
    "## call method above with a resume from test set and store result in bilou_df variable.\n",
    "resume = [ str(x_test[n_resume]), y_test[n_resume] ]\n",
    "bilou_df = make_bilou_df( nlp, resume )\n",
    "display(bilou_df)  "
   ],
   "execution_count": 31,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Predicted</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ananya</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chavan</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lecturer</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>635</th>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>636</th>\n",
       "      <td>information</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>637</th>\n",
       "      <td>of</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>638</th>\n",
       "      <td>Snakes</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>639</th>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>640 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Tokens Predicted True\n",
       "0         Ananya         O    O\n",
       "1         Chavan         O    O\n",
       "2                        O    O\n",
       "3       lecturer         O    O\n",
       "4              -         O    O\n",
       "..           ...       ...  ...\n",
       "635                      O    O\n",
       "636  information         O    O\n",
       "637           of         O    O\n",
       "638       Snakes         O    O\n",
       "639            .         O    O\n",
       "\n",
       "[640 rows x 3 columns]"
      ]
     },
     "metadata": {
      "tags": []
     }
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_EMld_CTuyTL"
   },
   "source": [
    "Inform yourself on the [BILUO](https://spacy.io/usage/linguistic-features#accessing-ner) scheme.<br>\n",
    "__Question 7__: Why do you think is it better to tag entities using this scheme (consider names of humans, descriptions of items in a shop)?<br>\n",
    "__Task 11__: employ pandas dataframe api to get a subset where predicted and true labels are the same. Compute the accuracy using the formula you researched above."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HWRI3IfluPD7",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "af568b15-3e47-44f8-a001-ea1759fcdb55"
   },
   "source": [
    "##  bilou_df is a pandas dataframe. Use pandas dataframe api to get a subset where predicted and true are the same. \n",
    "same_df = bilou_df[bilou_df[\"Predicted\"] == bilou_df[\"True\"]]\n",
    "##  compute the accuracy (n_correct/n_total)\n",
    "accuracy = len(same_df) / len(bilou_df)\n",
    "\n",
    "print(\"Accuracy on one resume: \",accuracy)"
   ],
   "execution_count": 32,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Accuracy on one resume:  0.7625\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1P4EgefioE-Q"
   },
   "source": [
    "The __accuracy__ is not 100%. Therefore, we want to have a look at those tokens, where the predicted and the true value differ.<br>\n",
    "__Task 12__: create a dataframe diff_df where the predicted values and the true values differ"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DdHFX1cMn-r6",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "outputId": "441e2fe6-75bf-4906-9d07-9475aca12809"
   },
   "source": [
    "# TODO create a dataframe diff_df where the predicted values and the true values differ\n",
    "diff_df = bilou_df[bilou_df[\"Predicted\"] != bilou_df[\"True\"]]\n",
    "display(diff_df)"
   ],
   "execution_count": 33,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tokens</th>\n",
       "      <th>Predicted</th>\n",
       "      <th>True</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>indeed.com/r/Ananya-</td>\n",
       "      <td>O</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Chavan/738779ab71971a96</td>\n",
       "      <td>O</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>Mumbai</td>\n",
       "      <td>O</td>\n",
       "      <td>B-College Name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>Versioning</td>\n",
       "      <td>O</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>Tools</td>\n",
       "      <td>O</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>352</th>\n",
       "      <td>:</td>\n",
       "      <td>O</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>353</th>\n",
       "      <td>Git</td>\n",
       "      <td>O</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>152 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Tokens Predicted            True\n",
       "17      indeed.com/r/Ananya-         O               -\n",
       "18                                   O               -\n",
       "19   Chavan/738779ab71971a96         O               -\n",
       "20                                   O               -\n",
       "185                   Mumbai         O  B-College Name\n",
       "..                       ...       ...             ...\n",
       "349                                  O               -\n",
       "350               Versioning         O               -\n",
       "351                    Tools         O               -\n",
       "352                        :         O               -\n",
       "353                      Git         O               -\n",
       "\n",
       "[152 rows x 3 columns]"
      ]
     },
     "metadata": {
      "tags": []
     }
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OWvyiCokonUI"
   },
   "source": [
    "Since we only considered one resumee, we now make this comparison for the whole test set.<br>\n",
    "__Task 13__: Complete the following code for the computation of the overall accuracy."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bidqT9GjovAg",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a9ee9554-0908-4f76-ccd8-3abf401949c9"
   },
   "source": [
    "doc_accuracy = []\n",
    "\n",
    "for i in range( len( x_test ) ):\n",
    "\n",
    "  resume = [x_test[i], y_test[i]]\n",
    "\n",
    "  bilou_df = make_bilou_df(nlp,resume)\n",
    "\n",
    "  same_df = bilou_df[bilou_df[\"Predicted\"] == bilou_df[\"True\"]]\n",
    "\n",
    "  doc_accuracy.append(  len(same_df) / len(bilou_df) )\n",
    "\n",
    "total_acc = np.mean( doc_accuracy )\n",
    "print(\"Accuracy: \",total_acc)"
   ],
   "execution_count": 34,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Accuracy:  0.8862631649960832\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cL7GE3RQpzbg"
   },
   "source": [
    "So we got an __accuracy__ of about 90% on average. This is quite good considering, that we only considered about 300 cases for training.<br>\n",
    "__Task 14__: Next, we want to find out, what the model did, when it went wrong. We only consider 5 resumees.<br>\n",
    "Complete the following code."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Y02Pkxriq8GM",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "692c2780-d94c-41cb-cad1-2d7392c0b6a1"
   },
   "source": [
    "for i in range( 5 ):\n",
    "\n",
    "  resume = [x_test[i], y_test[i]]\n",
    "\n",
    "  bilou_df = make_bilou_df(nlp,resume)\n",
    "\n",
    "  difference_df =  bilou_df[bilou_df[\"Predicted\"] != bilou_df[\"True\"]]\n",
    "\n",
    "  # print, where the labels from Spacy and the annotation differ. Print the text, the predicted and the true labels.\n",
    "\n",
    "  print(\"Text: \", resume[0])\n",
    "  print(\" ----------Predicted LAbel ------ \")\n",
    "  print(difference_df[\"Predicted\"] )\n",
    "  print(\" ----------True Label ------ \")\n",
    "  print(difference_df[\"True\"] )\n"
   ],
   "execution_count": 35,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Text:  Sudaya Puranik\n",
      "Principal Engineer Technical Staff - Company 1\n",
      "\n",
      "Bengaluru, Karnataka - Email me on Indeed: indeed.com/r/Sudaya-Puranik/eaf5f7c1a67c6c38\n",
      "\n",
      "To secure a promising position that offers both a challenge and a good opportunity for both\n",
      "professional and personal growth.\n",
      "\n",
      "Willing to relocate to: Bengaluru, Karnataka\n",
      "\n",
      "WORK EXPERIENCE\n",
      "\n",
      "Principal Engineer Technical Staff\n",
      "\n",
      "Company 1 -  Bengaluru, Karnataka -\n",
      "\n",
      "September 2005 to Present\n",
      "\n",
      "Total Experience: 12 years 6 months.\n",
      "Worked onshore and offshore for the projects. Extensive client facing and team management\n",
      "skills.\n",
      "\n",
      "Company 1: Oracle India Private Limited, Bangalore\n",
      "Designation: Principal Engineer Technical Staff\n",
      "Duration: September 2005 - till date\n",
      "\n",
      "KEY TECHNICAL SKILLS\n",
      "Databases: Oracle 9i/10g, MySql\n",
      "Languages: PL/SQL, Shell Script.\n",
      "Frameworks and Portals: Oracle ADF\n",
      "Server side Technology: J2ee,\n",
      "IDE: Jdeveloper 10.x\n",
      "\n",
      "TECHNICAL EXPERIENCE SUMMARY\n",
      "➢ 1 year Oracle Applications checklist implementation\n",
      "➢ 3+ years of Disaster Recovery on Oracle Applications (EBSO 11i, R12) and Database.\n",
      "➢ 8+ years of Oracle Applications DBA with Build and Release activities.\n",
      "➢ Installation of Oracle Applications (EBSO 11i, R12) at Customer environment and carry out DR\n",
      "➢ Configuration of Oracle Applications (EBSO 11i, R12) as per Customer specification.\n",
      "➢ Configuration of Discoverer, Admin, Concurrent, Forms, Web, Reports done on separate pillars\n",
      "➢ Patching, Cloning of Oracle Apps\n",
      "➢ Database conversion from non-RAC to 2 node, 3-node RAC\n",
      "➢ Installation of OTO product on customer environment. Including the Infra, Asmt, discoverer,\n",
      "OC4J product.\n",
      "➢ Carried out Disaster Recovery solutions for OTO\n",
      "➢ Database Switchover code written for OTO\n",
      "➢ Domain knowledge of Manual Testing.\n",
      "➢ Oracle Apps Release and Build Engineer,\n",
      "➢ Carried out 11i and R12 Upgrades on Oracle Internal Customers.\n",
      "➢ Carried out 12.2.X upgrades from 11i and R12 baselines with 11g and 12c database level\n",
      "\n",
      "https://www.indeed.com/r/Sudaya-Puranik/eaf5f7c1a67c6c38?isid=rex-download&ikw=download-top&co=IN\n",
      "\n",
      "\n",
      "➢ Experienced with Database upgrades as well. Carried out upgrades from 10g to 11g, 11g to\n",
      "12C DB.\n",
      "➢ Have been awarded Extra Mile Award 2 times for execution of projects on time with quality.\n",
      "\n",
      "Principal Engineer\n",
      "\n",
      "ORACLE -\n",
      "\n",
      "September 2005 to Present\n",
      "\n",
      "Oracle Apps DBA for R12 and 12.2.X baselines, responsible for all the EBS release activities ,\n",
      "handled upgrades with latest adop technology.\n",
      "\n",
      "EDUCATION\n",
      "\n",
      "Bachelor of Engineering (Information Science) in Information Science\n",
      "\n",
      "B V Bhoomaraddi College of Engg and Technology Hubli -  Hubli, Karnataka\n",
      "\n",
      "2005\n",
      "\n",
      "Visveswaraiah Technological University -  Belgaum, Karnataka\n",
      "\n",
      "SKILLS\n",
      "\n",
      "Oracle Apps DBA Release Enginner\n",
      "\n",
      "ADDITIONAL INFORMATION\n",
      "\n",
      "Software/Tools\n",
      "• Test cases written in MS Word\n",
      "• Carried out manually as QTP does not support this Application\n",
      "5) Disaster Recovery Portal for carrying out Recovery on the Customers.\n",
      "• It involves a portal through which we provide the Customer details and host types. There are\n",
      "xml plug-in, which are installed and parsed as Workflows. The workflows contain the steps of the\n",
      "DR for the MT and DB. As and when required we can pause and skip during the recovery. Tool\n",
      "tested and verified by the On Demand Operations and Customers like GE, Xerox, RIT etc.\n",
      "Client: On Demand Operations, Oracle Bangalore\n",
      "Roles and Responsibilities:\n",
      "• Developed all the plug-in needed for the recovery which include Database Plug-in, E-business\n",
      "Suite Plug-in.\n",
      "• Monitoring and Support 24/7\n",
      "\n",
      "Team Size: 2\n",
      "• Software /Tools: Shell and Expect scripting.\n",
      "\n",
      "4) DR Solution for R12 (E-Business Suite)\n",
      "It involves setting up the primary by installing Oracle Apps R12, setup the standby site i.e\n",
      "Database, Mid Tier, by cloning the same and recover. It also involves operations like Switch Over\n",
      "and Fail Over.\n",
      "By Switch Over we mean Primary to Standby and vice versa\n",
      "\n",
      "\n",
      "\n",
      "By Fail Over we mean the Primary has crashed, we need to make the Standby as the Primary.\n",
      "Client: On Demand Operations, Oracle Bangalore\n",
      "\n",
      "Team Size: 2\n",
      "Software /Tools: Shell and Expect scripting, PLSQL.\n",
      "\n",
      "Roles and Responsibilities:\n",
      "\n",
      "• Installation, Cloning, Patching of the Oracle Applications R12.\n",
      "• Conversion of Database from non-RAC to RAC\n",
      "• Setting up of the Standby.\n",
      "• Carry out the Switchover, Fail over and Recovery.\n",
      "• Carryout the whole Recovery Process through the UI.\n",
      "\n",
      "3) On Demand Service Continuity\n",
      "\n",
      "The project dealt with \"Disaster Recovery & Backup\" which involves automating checks so that\n",
      "disaster could be prevented. Also the recovery of database is involved.\n",
      "Client: On Demand Operations, Oracle Bangalore\n",
      "Team Size: 5\n",
      "Software /Tools:\n",
      "Unix Advanced shell scripting and expect. (Involves working on Korn Shell)\n",
      "\n",
      "Roles and Responsibilities:\n",
      "• Automated the checklists for E-business suite and PeopleSoft products.\n",
      "\n",
      "2) CCB Portal Administration\n",
      "\n",
      "Project Description\n",
      "\n",
      "The new CCB Administration component is an integrated, web-based application intended to\n",
      "consolidate and simplify the CCB processes for On Demand. This application:\n",
      "\n",
      "• Makes it easier to manage the process of change implementation in customer instances across\n",
      "the various milestones in the On Demand life cycle.\n",
      "\n",
      "• Simplifies said change implementation by keeping the information organized in a centralized\n",
      "repository.\n",
      "\n",
      "• In the CCB Administration application Release 1.0\n",
      "Client: System Assurance Centre, Oracle Unites States of America\n",
      "\n",
      "Team Size: 3\n",
      "Software /Tools:\n",
      "\n",
      "PL/SQL and HTML\n",
      "\n",
      "Roles and Responsibilities:\n",
      "\n",
      "\n",
      "\n",
      "* Developed the PL/SQL procedures & packages, HTML forms required for the Portal, using HTML\n",
      "for the user to enter the required data.\n",
      "* The data is entered is validated, using Java Scripting, and the data is stored in the database.\n",
      "* Developed the reports.\n",
      "\n",
      "1) ORACLE PRODUCTION ASSESSMENTS\n",
      "\n",
      "Project Description\n",
      "AUTO VERIFY TOOL\n",
      "Auto Verify is a utility that automates many of the tedious and repetitive tasks that are involved\n",
      "during QA testing of an Oracle Applications Release 11i environment. Auto Verify now performs\n",
      "many of the tasks automatically that used to be performed manually.\n",
      "Client: System Assurance Centre, Oracle Unites States of America\n",
      "\n",
      "Team Size: 4\n",
      "Software /Tools:\n",
      "UNIX Advanced Shell Scripting, PL/SQL.\n",
      "\n",
      "Roles and Responsibilities:\n",
      "Automated the all the checklists for 8 releases of E-business suite.\n",
      " ----------Predicted LAbel ------ \n",
      "20     O\n",
      "132    O\n",
      "133    O\n",
      "134    O\n",
      "135    O\n",
      "136    O\n",
      "137    O\n",
      "138    O\n",
      "139    O\n",
      "140    O\n",
      "141    O\n",
      "142    O\n",
      "143    O\n",
      "144    O\n",
      "145    O\n",
      "146    O\n",
      "147    O\n",
      "148    O\n",
      "149    O\n",
      "150    O\n",
      "151    O\n",
      "152    O\n",
      "153    O\n",
      "154    O\n",
      "155    O\n",
      "156    O\n",
      "157    O\n",
      "158    O\n",
      "159    O\n",
      "160    O\n",
      "161    O\n",
      "162    O\n",
      "163    O\n",
      "164    O\n",
      "165    O\n",
      "166    O\n",
      "167    O\n",
      "380    O\n",
      "477    O\n",
      "478    O\n",
      "479    O\n",
      "480    O\n",
      "481    O\n",
      "482    O\n",
      "483    O\n",
      "484    O\n",
      "485    O\n",
      "494    O\n",
      "495    O\n",
      "496    O\n",
      "497    O\n",
      "505    O\n",
      "506    O\n",
      "507    O\n",
      "508    O\n",
      "509    O\n",
      "510    O\n",
      "Name: Predicted, dtype: object\n",
      " ----------True Label ------ \n",
      "20     U-Email Address\n",
      "132           B-Skills\n",
      "133           I-Skills\n",
      "134           I-Skills\n",
      "135           I-Skills\n",
      "136           I-Skills\n",
      "137           I-Skills\n",
      "138           I-Skills\n",
      "139           I-Skills\n",
      "140           I-Skills\n",
      "141           I-Skills\n",
      "142           I-Skills\n",
      "143           I-Skills\n",
      "144           I-Skills\n",
      "145           I-Skills\n",
      "146           I-Skills\n",
      "147           I-Skills\n",
      "148           I-Skills\n",
      "149           I-Skills\n",
      "150           I-Skills\n",
      "151           I-Skills\n",
      "152           I-Skills\n",
      "153           I-Skills\n",
      "154           I-Skills\n",
      "155           I-Skills\n",
      "156           I-Skills\n",
      "157           I-Skills\n",
      "158           I-Skills\n",
      "159           I-Skills\n",
      "160           I-Skills\n",
      "161           I-Skills\n",
      "162           I-Skills\n",
      "163           I-Skills\n",
      "164           I-Skills\n",
      "165           I-Skills\n",
      "166           I-Skills\n",
      "167           L-Skills\n",
      "380                  -\n",
      "477     B-College Name\n",
      "478     I-College Name\n",
      "479     I-College Name\n",
      "480     I-College Name\n",
      "481     I-College Name\n",
      "482     I-College Name\n",
      "483     I-College Name\n",
      "484     I-College Name\n",
      "485     L-College Name\n",
      "494     B-College Name\n",
      "495     I-College Name\n",
      "496     I-College Name\n",
      "497     L-College Name\n",
      "505                  -\n",
      "506                  -\n",
      "507                  -\n",
      "508                  -\n",
      "509                  -\n",
      "510                  -\n",
      "Name: True, dtype: object\n",
      "Text:  Ashok Kunam\n",
      "Team Lead - Microsoft\n",
      "\n",
      "- Email me on Indeed: indeed.com/r/Ashok-Kunam/7aac8767aacf10a0\n",
      "\n",
      "• Software Engineering professional with over 2.5 years of experience in development,\n",
      "enhancement, integration, implementation and maintenance of software applications using\n",
      "Oracle\n",
      "• Served some of the major clients such as Microsoft, Cisco, Oracle Eloqua, iTalent\n",
      "• Knowledge of Operating Systems (OS) coding techniques, Internet Protocol (IP), interfaces and\n",
      "hardware subsystems, reading schematics and data sheets for components\n",
      "• Expertise in Java, J2EE, hibernate, spring, PHP, Nifi, Angular JS, JavaScript, JQuery, REST Web\n",
      "services, Jive, Lithium\n",
      "• Successfully developed cloud based connectors using REST Web services and JIVE custom tiles\n",
      "& plugins\n",
      "• Proficient in end-to-end implementation of various projects; including designing, development,\n",
      "coding, debugging, analyzing the logs & implementation of software applications\n",
      "• Proven skills of understanding business requirements and translating them into technical\n",
      "specifications\n",
      "• Possess interpersonal, analytical and negotiation skills with proven track record of utilizing a\n",
      "process-oriented approach towards the accomplishment of cost, profit, service & organizational\n",
      "goals\n",
      "\n",
      "WORK EXPERIENCE\n",
      "\n",
      "Team Lead\n",
      "\n",
      "Microsoft -\n",
      "\n",
      "July 2017 to Present\n",
      "\n",
      "• Environment: Java 1.8, Spring MVC, Hibernate, REST API Calls, Power BI desktop tool, Lithium\n",
      "Bulk data API, SQL Server, Maven, Log 4j, Git, JIRA, Scrum\n",
      "• Role: Team Lead\n",
      "• Period: July '17 - Till Date\n",
      "• Description: Power BI deployed on Lithium Community, 7, 000 Power BI users can collaborate\n",
      "and transfer the knowledge among them. On an average 150 k activity per day in the community.\n",
      "Cloud Connector is required to fetch the all the activities from the community and perform data\n",
      "mining and preserve into database as report generator format\n",
      "• Highlights:\n",
      "◦ Provided support in the total software development life cycle of the project\n",
      "◦ Implemented cloud connector to read the Lithium bulk data API, parse the data and inserted\n",
      "it into SQL Server\n",
      "◦ Generated views as per requirements & prepared Power BI reports\n",
      "\n",
      "• Organization: iTalent Corporation\n",
      "• Title: MyiTalent\n",
      "• Client: iTalent\n",
      "\n",
      "https://www.indeed.com/r/Ashok-Kunam/7aac8767aacf10a0?isid=rex-download&ikw=download-top&co=IN\n",
      "\n",
      "\n",
      "• Environment: Java 1.8, PHP, REST API Calls, hibernate, Angular JS, CSS, MySQL, Maven, Log\n",
      "4j, Git, JIRA, Linux\n",
      "• Role: Team Lead\n",
      "• Period: July '17 - Till Date\n",
      "• Description: Instantly find and hire the right candidate for your job opening. With MyiTalent,\n",
      "corporate human resource managers and hiring managers can now easily manage the hiring\n",
      "process with their recruiters. Review and select from pre-screened candidates that match your\n",
      "requirements. Find and identify experts within your company. Browse project scope of work and\n",
      "details, including budget information. Collaborate with your own personal recruiter at all times.\n",
      "Provide feedback on candidates and projects\n",
      "• Highlights:\n",
      "◦ Developed PHP Backend Services\n",
      "◦ Implemented ZOHO integration for jobs and candidates using REST web services\n",
      "◦ Created Cron Job to fetch candidates from ZOHO\n",
      "\n",
      "• Organization: iTalent Corporation\n",
      "• Title: CSC\n",
      "• Client: Cisco\n",
      "• Environment: Java 1.8, Spring MVC, Hibernate, REST API Calls, Lithium Bulk data API, MySQL,\n",
      "Maven, Log 4j, Git, JIRA, Scrum\n",
      "• Role: Team Lead\n",
      "• Period: July '17 - Till Date\n",
      "• Description: Migration from Drupal community to the Lithium Community which is enhanced\n",
      "with custom cloud search using Attivio.\n",
      "• Highlights:\n",
      "◦ Provided support in the total software development life cycle of the project\n",
      "◦ Implemented cloud connector to read the Lithium bulk data API, parse the data and inserted\n",
      "it into SQL Server\n",
      "◦ Generated views as per requirements & prepared Power BI reports\n",
      "\n",
      "EDUCATION\n",
      "\n",
      "Bachelor of Technology in ECE\n",
      "\n",
      "Jawaharlal Nehru Technology University -  Kakinada, Andhra Pradesh\n",
      "\n",
      "2014\n",
      "\n",
      "SKILLS\n",
      "\n",
      "database (Less than 1 year), Git (Less than 1 year), Java (Less than 1 year), JIRA (Less than 1\n",
      "year), life cycle (Less than 1 year)\n",
      "\n",
      "LINKS\n",
      "\n",
      "https://www.linkedin.com/in/ashok-kunam-85a845a8\n",
      "\n",
      "https://www.linkedin.com/in/ashok-kunam-85a845a8\n",
      "\n",
      "\n",
      "ADDITIONAL INFORMATION\n",
      "\n",
      "Technical Skills\n",
      "\n",
      "• Programming Languages: Java & J2EE, PHP, Apache Nifi, SOAP Web Services, Rest Web Services,\n",
      "Jackson-2\n",
      "• Framework's: Spring, Hibernate, Junit, Mockito\n",
      "• Database: MYSQL, SQL Server, Oracle\n",
      "• App/Web Servers: Tomcat, Apache\n",
      "\n",
      "• Operating Systems: Windows, Linux\n",
      "• SKM: Lithium\n",
      "• Tools: Git, JIRA, Eclipse, putty\n",
      "\n",
      "Knowledge Purview\n",
      "\n",
      "• Methodologies and Principles:\n",
      "◦ Code Generation Tools\n",
      "◦ Documentation Generation\n",
      "◦ Inversion of Control\n",
      "◦ Iterative Development\n",
      "◦ Object Oriented Programming\n",
      "◦ Deployment\n",
      "◦ Agile\n",
      "\n",
      "Professional Experience\n",
      "\n",
      "Core Competencies\n",
      "\n",
      "~ Requirement Gathering\n",
      "~ Deployment & Support\n",
      "~ Project Management\n",
      "~ Software Development Life Cycle\n",
      "~ Delivery Management\n",
      "~ Quality Assurance\n",
      "~ Application / Software Development\n",
      "~ Liaison & Coordination\n",
      " ----------Predicted LAbel ------ \n",
      "14     O\n",
      "428    O\n",
      "753    O\n",
      "754    O\n",
      "755    O\n",
      "      ..\n",
      "978    O\n",
      "979    O\n",
      "980    O\n",
      "981    O\n",
      "982    O\n",
      "Name: Predicted, Length: 208, dtype: object\n",
      " ----------True Label ------ \n",
      "14     U-Email Address\n",
      "428                  -\n",
      "753     B-College Name\n",
      "754     I-College Name\n",
      "755     I-College Name\n",
      "            ...       \n",
      "978           I-Skills\n",
      "979           I-Skills\n",
      "980           I-Skills\n",
      "981           I-Skills\n",
      "982           L-Skills\n",
      "Name: True, Length: 208, dtype: object\n",
      "Text:  Ammit Sharma\n",
      "General Manager - Sales & Marketing - Thakkar Stardom Events Pvt. Ltd\n",
      "\n",
      "Mumbai, Maharashtra - Email me on Indeed: indeed.com/r/Ammit-Sharma/1ded1fcc57236d58\n",
      "\n",
      "Having 13+ years of experience with a progressive organization that gives me scope to apply my\n",
      "knowledge and skills coupled with patience and to get involved as part of a team that dynamically\n",
      "work towards the growth of the Company.\n",
      "\n",
      "Willing to relocate to: Mumbai, Maharashtra - Hyderabad, Telangana - Jaipur, Rajasthan\n",
      "\n",
      "WORK EXPERIENCE\n",
      "\n",
      "General Manager - Sales & Marketing\n",
      "\n",
      "Thakkar Stardom Events Pvt. Ltd -\n",
      "\n",
      "September 2011 to Present\n",
      "\n",
      "Roles & Responsibilities:\n",
      "➢ Handling a Sales Team of 8 people for Golden Leaf Banquet. Taking care of Inbound &\n",
      "Outbound Sales. Handling segments like Banquet, Catering, Confectionary, Decorations, Event\n",
      "Management.\n",
      "➢ Handling Sales for Green Leaf Caterers (Outdoor Catering)\n",
      "➢ Tie-ups done with couple of new Projects like FM Banquet, Megarugas Banquet & many\n",
      "more\n",
      "➢ Handling Outbound sales for Golden Leaf Banquet & getting business for Cocktail Parties,\n",
      "Anniversaries, Birthday Parties, Weddings & Receptions, Conferences, Inductions, Product\n",
      "Launches, Seminars from companies like JP Morgan, Tech Mahindra, Sutherland, Bank of America,\n",
      "Ocwen, Technimont, Just Dial, Lakozy Toyota and many more\n",
      "➢ Doing Corporate Tie-ups with Event Management Firms, Production Houses, Pharmaceutical\n",
      "firms, BPO's, Bankings, etc\n",
      "➢ Maintaining good relationship with Clients as well as other department associates\n",
      "➢ Rollout the business MIS and analysis for improving revenue\n",
      "➢ Daily meetings and attending the hurdles for developing business\n",
      "➢ Daily follow up & tracking enquiries for a good 75% conversion overall\n",
      "➢ Conducting team reviews to achieve monthly budgets & targets\n",
      "➢ Achieved Business Targets for Banquet last year which is over & above the target given\n",
      "➢ Preparing the Standard Operating Procedures for the Banquets\n",
      "➢ Hiring and Training the right Candidates for Sales & other Departments\n",
      "➢ Responsible for Business planning, strategies, future sales forecast, marketing budgets\n",
      "➢ Preparing Incentive structure for my Sales Team\n",
      "➢ Tracking the Average Package Rates & Footfalls\n",
      "➢ Handling day to day inwards & outwards (Payment Flow)\n",
      "➢ Maintaining Guest Relation & Customer Satisfaction\n",
      "➢ Designing Packages & Menu along with marketing artworks\n",
      "➢ Reports to be maintained for the Business generated & growth\n",
      "\n",
      "Senior Business Development Executive\n",
      "\n",
      "https://www.indeed.com/r/Ammit-Sharma/1ded1fcc57236d58?isid=rex-download&ikw=download-top&co=IN\n",
      "\n",
      "\n",
      "JMJ Entertainments Pvt. Ltd -\n",
      "\n",
      "May 2011 to September 2011\n",
      "\n",
      "Roles & Responsibilities:-\n",
      "➢ Responsible for increasing the sourcing of lead generation through cold calling, referrals etc.\n",
      "➢ Responsible for bringing in new customers and boosting the profits.\n",
      "➢ Attempting to resolve issues and problems with customer's on floor.\n",
      "➢ Implementing the sales promotion programs.\n",
      "➢ Maintaining good relation with the customers.\n",
      "➢ Responsible for Corporate Tie-ups to increase the business\n",
      "\n",
      "Resolution Expert\n",
      "\n",
      "SUTHERLAND GLOBAL SERVICES PVT Ltd -\n",
      "\n",
      "September 2008 to May 2011\n",
      "\n",
      "Roles & Responsibilities:\n",
      "➢ Handling a Team of 15 Associates & Maintaining their Report.\n",
      "➢ Giving a Complete End to End Resolution on Call by Taking a Remote Access on the Customers\n",
      "System.\n",
      "➢ Assisting the customer claim process\n",
      "➢ Serving the existing and new customers in time\n",
      "➢ Providing Resolution & Solving Technical Queries Related to Hardware & Software\n",
      "➢ Resolves all customer query and escalations accordingly\n",
      "➢ Line of Support Includes Windows XP, VISTA & Windows 7 Operating System\n",
      "\n",
      "❖ Worked as a Sr. Customer Relationship Advisor in 3 GLOBAL SERVICES PVT Ltd, from Nov 2006\n",
      "to July 2008.\n",
      "Roles & Responsibilities:\n",
      "➢ Taking care of Inbound & Outbound Calls on the Floor\n",
      "➢ Achieving Metrics set by the Operations for the Floor\n",
      "➢ Active participation in pilot batch during the testing of the New Outbound Dialer\n",
      "➢ Collections to be made as per the targets\n",
      "\n",
      "Customer Relationship Advisor\n",
      "\n",
      "ZENTA PVT Ltd -\n",
      "\n",
      "August 2005 to November 2006\n",
      "\n",
      "Roles & Responsibilities:\n",
      "➢ Taking care of Inbound & Outbound Calls on the Floor\n",
      "➢ Achieving Metrics set by the Operations for the Floor\n",
      "➢ Active participation in pilot batch during the testing of the New Outbound Dialer\n",
      "➢ Collections to be made as per the targets\n",
      "➢ Interactions with US customers\n",
      "➢ Credit Card Collections\n",
      "\n",
      "System Engineer\n",
      "\n",
      "\n",
      "\n",
      "NEXPRO SOLUTIONS Ltd -  Mumbai, Maharashtra -\n",
      "\n",
      "October 2004 to September 2005\n",
      "\n",
      "Roles & Responsibilities:\n",
      "➢ Credit Card Data Centre at ICICI Infotech\n",
      "➢ Achieving Metrics set by the Operations for the Floor\n",
      "➢ Processing Data of VISA & MASTERCARD\n",
      "➢ Uploading & Downloading the Mastercard data files & Configuring Modem for JFT\n",
      "➢ Involved in resolving problems concerning authorizations of credit cards in live environment\n",
      "whenever any transactions declines.\n",
      "➢ Maintaining Credit Cards Production Server & Development Server.\n",
      "➢ Taking care of SRM server issues connected to HSM. Testing of incoming / outgoing transactions\n",
      "from / to VISA Singapore and ICICI Mumbai.\n",
      "\n",
      "EDUCATION\n",
      "\n",
      "M.B.A\n",
      "\n",
      "Venkatesh University\n",
      "\n",
      "Bachelors in Commerce\n",
      "\n",
      "Mumbai University -  Mumbai, Maharashtra\n",
      "\n",
      "H.S.C\n",
      "\n",
      "Mumbai University -  Mumbai, Maharashtra\n",
      "\n",
      "S.S.C\n",
      "\n",
      "Mumbai University -  Mumbai, Maharashtra\n",
      "\n",
      "SKILLS\n",
      "\n",
      "MICROSOFT WINDOWS (Less than 1 year)\n",
      "\n",
      "ADDITIONAL INFORMATION\n",
      "\n",
      "Technical Skills:\n",
      "Operating Systems: Microsoft Windows\n",
      " ----------Predicted LAbel ------ \n",
      "26     O\n",
      "459    O\n",
      "947    O\n",
      "948    O\n",
      "954    O\n",
      "955    O\n",
      "964    O\n",
      "965    O\n",
      "974    O\n",
      "975    O\n",
      "984    O\n",
      "985    O\n",
      "Name: Predicted, dtype: object\n",
      " ----------True Label ------ \n",
      "26     U-Email Address\n",
      "459                  -\n",
      "947     B-College Name\n",
      "948     L-College Name\n",
      "954     B-College Name\n",
      "955     L-College Name\n",
      "964     B-College Name\n",
      "965     L-College Name\n",
      "974     B-College Name\n",
      "975     L-College Name\n",
      "984           B-Skills\n",
      "985           L-Skills\n",
      "Name: True, dtype: object\n",
      "Text:  Ananya Chavan\n",
      "lecturer - oracle tutorials\n",
      "\n",
      "Mumbai, Maharashtra - Email me on Indeed: indeed.com/r/Ananya-\n",
      "Chavan/738779ab71971a96\n",
      "\n",
      "Seeking a responsible job with an opportunity for professional challenges and utilize my skills\n",
      "up to its extreme.\n",
      "\n",
      "WORK EXPERIENCE\n",
      "\n",
      "lecturer\n",
      "\n",
      "Oracle tutorials -  Mumbai, Maharashtra -\n",
      "\n",
      "April 2016 to Present\n",
      "\n",
      "for computer science (STD 11th and 12th) (2 years)\n",
      "➢ Worked at \"Dr.Babasaheb Ambedkar College, Chembur (W) \" as a lecturer for • B.Sc. (Computer\n",
      "Science & Information Technology)\n",
      "• F.Y.J.C. (Computer Science & I.T.)\n",
      "• S.Y.J.C. (Computer Science & I.T.)\n",
      "➢ Worked at \"LIVE\" as a Head of the IT Department and Lecturer for Web designing.\n",
      "➢ Worked at \"Kohinoor College Of Hotel Management\" as visiting lecturer for SEM I.\n",
      "➢ Working at \"ORACLE TUTORIALS\" as a lecturer for computer science (STD 11th and 12th)\n",
      "\n",
      "EDUCATION\n",
      "\n",
      "MCA\n",
      "\n",
      "Mumbai University -  Mumbai, Maharashtra\n",
      "\n",
      "B.Sc. in Com.Sci\n",
      "\n",
      "Mumbai University -  Mumbai, Maharashtra\n",
      "\n",
      "SKILLS\n",
      "\n",
      "SEARCH ENGINE MARKETING (2 years), SEM (2 years), ACCESS (Less than 1 year), AJAX (Less\n",
      "than 1 year), APACHE (Less than 1 year)\n",
      "\n",
      "ADDITIONAL INFORMATION\n",
      "\n",
      "Technical skills:\n",
      "Languages: C, C++, Java (J2EE),\n",
      "Web Component APIS:: Jdbc, Servlet, JSP.\n",
      "Frameworks: Spring 4 & Struts 2\n",
      "ORM Framework: Hibernate\n",
      "Web Development: Html5, CSS3, Java Script, Ajax &JQuery, Angular Js\n",
      "\n",
      "https://www.indeed.com/r/Ananya-Chavan/738779ab71971a96?isid=rex-download&ikw=download-top&co=IN\n",
      "https://www.indeed.com/r/Ananya-Chavan/738779ab71971a96?isid=rex-download&ikw=download-top&co=IN\n",
      "\n",
      "\n",
      "Application Servers: Apache Tomcat,\n",
      "IDE: Eclipse, Netbeans\n",
      "Database: Ms-Access, Mysql\n",
      "Operating Systems: Windows 7, 8, 10\n",
      "FTP Client: Filezilla\n",
      "Versioning Tools: Git\n",
      "\n",
      "Project Details:\n",
      "\n",
      "\"Real Estate Application\" (Client: Global Realtor PVT. LTD Pune)\n",
      "Front-End: Java (J2EE), JDBC, Servlet, JSP, Jquery.\n",
      "Back end: Mysql.\n",
      "Duration: 6 Month (Internship)\n",
      "Company Name: AryanTech India Pvt. Ltd. Pune\n",
      "My Role: Developer as Trainee.\n",
      "Module: Module 4.\n",
      "Description: Developed as a MCA Final SEM Project for\n",
      "\"Global Realtors PVT.LTD, Hinjewadi, Pune.\"\n",
      "The Real Estate Web Application is an interactive, effective and revenue-generating website\n",
      "designed for the Real Estate Industry. The main objective of this application is to help the Real\n",
      "Estate Company to display unlimited number of property listings on the website.\n",
      "\n",
      "\"Beauty Parlor Management System\" (B.Sc. (Com.Sci.))\n",
      "Tool: VB 6.0\n",
      "Language: VB\n",
      "Database: MS-Access\n",
      "Operating System: Windows XP\n",
      "The Beauty Parlor Management System is an easy and effective system to use. The main features\n",
      "of this system are to avoid manual work and keep storing all appointments of customers.\n",
      "\n",
      "\"Web Designing Project (Reptiles.com) \" (B.Sc. (Com.Sci.))\n",
      "Language: HTML and ASP\n",
      "Tool: Dreamweaver 8.0\n",
      "Database: MS-Access\n",
      "Operating System: Windows XP\n",
      "The Reptiles.com is a simple informative site. The main features of this system are to give all\n",
      "information of Snakes.\n",
      " ----------Predicted LAbel ------ \n",
      "17     O\n",
      "18     O\n",
      "19     O\n",
      "20     O\n",
      "185    O\n",
      "      ..\n",
      "349    O\n",
      "350    O\n",
      "351    O\n",
      "352    O\n",
      "353    O\n",
      "Name: Predicted, Length: 152, dtype: object\n",
      " ----------True Label ------ \n",
      "17                  -\n",
      "18                  -\n",
      "19                  -\n",
      "20                  -\n",
      "185    B-College Name\n",
      "            ...      \n",
      "349                 -\n",
      "350                 -\n",
      "351                 -\n",
      "352                 -\n",
      "353                 -\n",
      "Name: True, Length: 152, dtype: object\n",
      "Text:  VARUN AHLUWALIA\n",
      "Quantitative Analyst\n",
      "\n",
      "- Email me on Indeed: indeed.com/r/VARUN-AHLUWALIA/725d9b113f3c4f0c\n",
      "\n",
      "WORK EXPERIENCE\n",
      "\n",
      "Software Engineer\n",
      "\n",
      "Tavant -  Bangalore, Karnataka -\n",
      "\n",
      "April 2005 to April 2006\n",
      "\n",
      "• Implemented online retail sale management solution for industrial manufacturing\n",
      "giant Ingersoll Rand.\n",
      "• Implemented secondary mortgage solution for leading wholesale lender Ameriquest.\n",
      "\n",
      "Software Engineer\n",
      "\n",
      "Patni -  Bangalore, Karnataka -\n",
      "\n",
      "August 2004 to April 2005\n",
      "\n",
      "• Built profitability reports for using Oracle Financial Analytics\n",
      "\n",
      "EDUCATION\n",
      "\n",
      "Master of Science in Financial Mathematics\n",
      "\n",
      "University of Chicago -  Chicago, IL\n",
      "\n",
      "June 2010\n",
      "\n",
      "Bachelor of Technology in Civil Engineering\n",
      "\n",
      "Indian Institutes of Technology IIT Kanpur\n",
      "\n",
      "July 2004\n",
      "\n",
      "ADDITIONAL INFORMATION\n",
      "\n",
      "SKILLS\n",
      "Programming • JAVA, C++, C, Matlab, SQL\n",
      "\n",
      "Operating System • Windows, Linux\n",
      "\n",
      "https://www.indeed.com/r/VARUN-AHLUWALIA/725d9b113f3c4f0c?isid=rex-download&ikw=download-top&co=IN\n",
      " ----------Predicted LAbel ------ \n",
      "12     O\n",
      "13     O\n",
      "98     O\n",
      "99     O\n",
      "100    O\n",
      "117    O\n",
      "118    O\n",
      "119    O\n",
      "120    O\n",
      "121    O\n",
      "122    O\n",
      "132    O\n",
      "133    O\n",
      "134    O\n",
      "135    O\n",
      "136    O\n",
      "137    O\n",
      "138    O\n",
      "139    O\n",
      "140    O\n",
      "141    O\n",
      "142    O\n",
      "143    O\n",
      "144    O\n",
      "145    O\n",
      "146    O\n",
      "147    O\n",
      "148    O\n",
      "149    O\n",
      "150    O\n",
      "Name: Predicted, dtype: object\n",
      " ----------True Label ------ \n",
      "12                  -\n",
      "13                  -\n",
      "98     B-College Name\n",
      "99     I-College Name\n",
      "100    L-College Name\n",
      "117    B-College Name\n",
      "118    I-College Name\n",
      "119    I-College Name\n",
      "120    I-College Name\n",
      "121    I-College Name\n",
      "122    L-College Name\n",
      "132                 -\n",
      "133                 -\n",
      "134                 -\n",
      "135                 -\n",
      "136                 -\n",
      "137                 -\n",
      "138                 -\n",
      "139                 -\n",
      "140                 -\n",
      "141                 -\n",
      "142                 -\n",
      "143                 -\n",
      "144                 -\n",
      "145                 -\n",
      "146                 -\n",
      "147                 -\n",
      "148                 -\n",
      "149                 -\n",
      "150                 -\n",
      "Name: True, dtype: object\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0r21TdjPrt7P"
   },
   "source": [
    "__Question 8__: What was predicted, when the prediction differed from the true label?<br>\n",
    "What do you think is necessary for computing the accuracy on token level?<br> \n",
    "What is the advantage of computing the accuracy on token level?<br>\n",
    "__Task 15__: Complete the following code."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GZGOVg2U20V1",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 199
    },
    "outputId": "e3f4d27d-eaeb-48bf-97f9-7eae8fd798f3"
   },
   "source": [
    "##  cycle through chosen_entity_labels and calculate metrics for each entity using test data\n",
    "data = []\n",
    "for label in chosen_entity_labels:\n",
    "    ## variables to store results for all resumes for one entity type\n",
    "    true_positives = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    for i in range( len( x_test ) ):\n",
    "        ## use make_bilou_df on each resume in our test set, and calculate for each entity true and false positives,\n",
    "        ## and false negatives. \n",
    "\n",
    "        resume = [x_test[i], y_test[i]]\n",
    "        \n",
    "        tres_df = make_bilou_df(nlp,resume)\n",
    "        tp, fp, fn = 0,0,0\n",
    "        \n",
    "        ## calculate true false positives and false negatives for each resume\n",
    "        for k, items in tres_df.iterrows():\n",
    "          if items['True'] == items['Predicted'] and  label in items['True']: \n",
    "            tp += 1 \n",
    "          elif items['True'] != items['Predicted'] and  label in items['Predicted']: \n",
    "            fp += 1\n",
    "          elif items['True'] != items['Predicted'] and  label in items['True']:\n",
    "            fn += 1\n",
    "\n",
    "        ## aggregate result for each resume to totals\n",
    "        true_positives = true_positives + tp\n",
    "        false_positives = false_positives + fp\n",
    "        false_negatives = false_negatives + fn\n",
    "    \n",
    "    print(\"For label '{}' tp: {} fp: {} fn: {}\".format(label,true_positives,false_positives,false_negatives))\n",
    "    \n",
    "    ## Use the formulas you learned to calculate metrics and print them out\n",
    "    ## also: prevent division by zero without raising errors. Explain your choice\n",
    "    eps = 10e-8\n",
    "    precision = true_positives / (true_positives + false_positives + eps)\n",
    "    recall = true_positives / (true_positives + false_negatives + eps)\n",
    "    f1 = 2 * (precision * recall)/(precision + recall + eps)\n",
    "    \n",
    "    row = [label,precision,recall,f1]\n",
    "    data.append(row)\n",
    "\n",
    "## make pandas dataframe with metrics data. Use the chosen entity labels as an index, and the metric names as columns. \n",
    "metric_df = pd.DataFrame( data, columns = [ \"Label\", \"Precision\", \"Recall\", \"F1\" ] )\n",
    "display(metric_df)"
   ],
   "execution_count": 36,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "For label 'Skills' tp: 0 fp: 0 fn: 2972\n",
      "For label 'College Name' tp: 0 fp: 0 fn: 529\n",
      "For label 'Email Address' tp: 0 fp: 0 fn: 120\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Skills</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>College Name</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Email Address</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Label  Precision  Recall   F1\n",
       "0         Skills        0.0     0.0  0.0\n",
       "1   College Name        0.0     0.0  0.0\n",
       "2  Email Address        0.0     0.0  0.0"
      ]
     },
     "metadata": {
      "tags": []
     }
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rMKMlJswAqUZ"
   },
   "source": [
    "__Question 9__: Explain from these statistics how well __nlp__ performs.<br>\n",
    "__Task 16__: Compute for each metric (Precision, Recall, F1) the mean values."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5_gEZTQy5KTr",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "b736f6b7-0ae8-4665-f76f-6f9f62fc3178"
   },
   "source": [
    "for label in [ \"Precision\", \"Recall\", \"F1\" ]:\n",
    "    # Compute mean and print\n",
    "    print(label, \"\\t: \", metric_df[label].mean())"
   ],
   "execution_count": 37,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Precision \t:  0.0\n",
      "Recall \t:  0.0\n",
      "F1 \t:  0.0\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Q1rvvhfBoN9"
   },
   "source": [
    "__Question 10__: What do you learn, when you compare the performance of the model on the token level with the performance of the model on the global level from above?<br>\n",
    "Next, we prepare data for flair."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7vkzBYIlDbLe"
   },
   "source": [
    "train = [ [ x_train[ i ], y_train[ i ] ] for i in range( len( x_train ) ) ]\n",
    "\n",
    "test = [ [ x_test[ i ], y_test[ i ] ] for i in range( len( x_test ) ) ]"
   ],
   "execution_count": 38,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L_zsjjgmuyTQ"
   },
   "source": [
    "__Task 17__: Complete the following code."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NBfdFr1qNKBv"
   },
   "source": [
    "\n",
    "# prepare data\n",
    "training_data_as_bilou = [make_bilou_df(nlp,res) for res in train]\n",
    "\n",
    "test_data_as_bilou = [make_bilou_df(nlp,res) for res in test]\n",
    "\n",
    "\n",
    "# set up paths\n",
    "path_to_training_file = os.getcwd() + \"/training_data.csv\"\n",
    "\n",
    "path_to_test_file = os.getcwd() + \"/test_data.csv\"\n",
    "\n",
    "\n",
    "\n",
    "# make sure, that if the corresponding files exist, they are emptied\n",
    "if os.path.isfile( path_to_training_file ):\n",
    "\n",
    "  open( path_to_training_file, \"w\" ).close()\n",
    "\n",
    "if os.path.isfile( path_to_test_file ):\n",
    "\n",
    "  open( path_to_test_file, \"w\" ).close()\n",
    "\n",
    "\n",
    "# open empty files\n",
    "training_file = open( path_to_training_file, \"a\", encoding = \"utf-8\" )\n",
    "    \n",
    "test_file = open( path_to_test_file, \"a\", encoding = \"utf-8\" )\n",
    "\n",
    "\n",
    "for item in training_data_as_bilou:\n",
    "  for i, row in item.iterrows():\n",
    "      token = row[\"Tokens\"]\n",
    "      #  remove all tokens like \"\", \" \", \"\\n\" by ignoring them\n",
    "      if token in [\"\", \" \", \"\\n\"]:\n",
    "          continue\n",
    "\n",
    "      # for all other tokens do the following:\n",
    "      label =row[\"True\"]\n",
    "      # create a string s: s = token + \" \" + label + \"\\n\"\n",
    "      s = token + \" \" \n",
    " \n",
    "      # if the label is \"-\", then write s = token + \" O\\n\"\n",
    "      s = s + (label if label != \"-\" else \"0\") + \"\\n\"\n",
    "      # write this newly created string to file\n",
    "      training_file.write(s)\n",
    "      # if this newly created string contains \".\", then also write a\n",
    "      # newline to file that only contains \"\\n\"\n",
    "      if \".\" in s:\n",
    "        training_file.write(\"\\n\")\n",
    "\n",
    "  # Using this scheme, each line in the resulting files corresponds either to an empty line or a token.\n",
    "  # Flair assembles a block of nonempty lines into a sentence. Therefore, the empty line\n",
    "  # is a signal for Flair that the current sentence is finished. Therefore, we extracted\n",
    "  # the whitespaces above.\n",
    "\n",
    "for item in test_data_as_bilou:\n",
    "  #  the same as above.\n",
    "  for i, row in item.iterrows():\n",
    "    label = row[\"True\"]\n",
    "    token = row[\"Tokens\"]\n",
    "    if token in [\"\", \" \", \"\\n\"]:\n",
    "        continue\n",
    "    s = token + \" \" + (label if label != \"-\" else \"0\") + \"\\n\"\n",
    "    test_file.write(s)\n",
    "    if \".\" in s:\n",
    "      test_file.write(\"\\n\")\n",
    "\n",
    "training_file.close()\n",
    "\n",
    "test_file.close()\n"
   ],
   "execution_count": 39,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-V5trnr4gXa9"
   },
   "source": [
    "Start Flair"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "630pwinWhXWo",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "9176c16e-825f-4fd8-f855-7e82aff37c0c"
   },
   "source": [
    "pip install flair"
   ],
   "execution_count": 40,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Collecting flair\n",
      "\u001B[?25l  Downloading https://files.pythonhosted.org/packages/f0/3a/1b46a0220d6176b22bcb9336619d1731301bc2c75fa926a9ef953e6e4d58/flair-0.8.0.post1-py3-none-any.whl (284kB)\n",
      "\r\u001B[K     |█▏                              | 10kB 23.2MB/s eta 0:00:01\r\u001B[K     |██▎                             | 20kB 27.0MB/s eta 0:00:01\r\u001B[K     |███▌                            | 30kB 30.4MB/s eta 0:00:01\r\u001B[K     |████▋                           | 40kB 32.3MB/s eta 0:00:01\r\u001B[K     |█████▊                          | 51kB 32.6MB/s eta 0:00:01\r\u001B[K     |███████                         | 61kB 34.6MB/s eta 0:00:01\r\u001B[K     |████████                        | 71kB 34.0MB/s eta 0:00:01\r\u001B[K     |█████████▏                      | 81kB 30.9MB/s eta 0:00:01\r\u001B[K     |██████████▍                     | 92kB 31.7MB/s eta 0:00:01\r\u001B[K     |███████████▌                    | 102kB 32.3MB/s eta 0:00:01\r\u001B[K     |████████████▋                   | 112kB 32.3MB/s eta 0:00:01\r\u001B[K     |█████████████▉                  | 122kB 32.3MB/s eta 0:00:01\r\u001B[K     |███████████████                 | 133kB 32.3MB/s eta 0:00:01\r\u001B[K     |████████████████                | 143kB 32.3MB/s eta 0:00:01\r\u001B[K     |█████████████████▎              | 153kB 32.3MB/s eta 0:00:01\r\u001B[K     |██████████████████▍             | 163kB 32.3MB/s eta 0:00:01\r\u001B[K     |███████████████████▋            | 174kB 32.3MB/s eta 0:00:01\r\u001B[K     |████████████████████▊           | 184kB 32.3MB/s eta 0:00:01\r\u001B[K     |█████████████████████▉          | 194kB 32.3MB/s eta 0:00:01\r\u001B[K     |███████████████████████         | 204kB 32.3MB/s eta 0:00:01\r\u001B[K     |████████████████████████▏       | 215kB 32.3MB/s eta 0:00:01\r\u001B[K     |█████████████████████████▎      | 225kB 32.3MB/s eta 0:00:01\r\u001B[K     |██████████████████████████▌     | 235kB 32.3MB/s eta 0:00:01\r\u001B[K     |███████████████████████████▋    | 245kB 32.3MB/s eta 0:00:01\r\u001B[K     |████████████████████████████▊   | 256kB 32.3MB/s eta 0:00:01\r\u001B[K     |██████████████████████████████  | 266kB 32.3MB/s eta 0:00:01\r\u001B[K     |███████████████████████████████ | 276kB 32.3MB/s eta 0:00:01\r\u001B[K     |████████████████████████████████| 286kB 32.3MB/s \n",
      "\u001B[?25hCollecting sentencepiece==0.1.95\n",
      "\u001B[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
      "\u001B[K     |████████████████████████████████| 1.2MB 37.3MB/s \n",
      "\u001B[?25hCollecting deprecated>=1.2.4\n",
      "  Downloading https://files.pythonhosted.org/packages/fb/73/994edfcba74443146c84b91921fcc269374354118d4f452fb0c54c1cbb12/Deprecated-1.2.12-py2.py3-none-any.whl\n",
      "Collecting bpemb>=0.3.2\n",
      "  Downloading https://files.pythonhosted.org/packages/f2/6f/9191b85109772636a8f8accb122900c34db26c091d2793218aa94954524c/bpemb-0.3.3-py3-none-any.whl\n",
      "Requirement already satisfied: gensim<=3.8.3,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from flair) (3.6.0)\n",
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from flair) (0.8.9)\n",
      "Collecting konoha<5.0.0,>=4.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/71/70/48a0bd55f79c328504fe6fe7ae8ff651f77a2aadbb1911701385d9bb5ca3/konoha-4.6.5-py3-none-any.whl\n",
      "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from flair) (2019.12.20)\n",
      "Collecting janome\n",
      "\u001B[?25l  Downloading https://files.pythonhosted.org/packages/a8/63/98858cbead27df7536c7e300c169da0999e9704d02220dc6700b804eeff0/Janome-0.4.1-py2.py3-none-any.whl (19.7MB)\n",
      "\u001B[K     |████████████████████████████████| 19.7MB 153kB/s \n",
      "\u001B[?25hRequirement already satisfied: hyperopt>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from flair) (0.1.2)\n",
      "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.7/dist-packages (from flair) (3.2.2)\n",
      "Collecting gdown==3.12.2\n",
      "  Downloading https://files.pythonhosted.org/packages/50/21/92c3cfe56f5c0647145c4b0083d0733dd4890a057eb100a8eeddf949ffe9/gdown-3.12.2.tar.gz\n",
      "  Installing build dependencies ... \u001B[?25l\u001B[?25hdone\n",
      "  Getting requirements to build wheel ... \u001B[?25l\u001B[?25hdone\n",
      "    Preparing wheel metadata ... \u001B[?25l\u001B[?25hdone\n",
      "Collecting torch<=1.7.1,>=1.5.0\n",
      "\u001B[?25l  Downloading https://files.pythonhosted.org/packages/90/5d/095ddddc91c8a769a68c791c019c5793f9c4456a688ddd235d6670924ecb/torch-1.7.1-cp37-cp37m-manylinux1_x86_64.whl (776.8MB)\n",
      "\u001B[K     |████████████████████████████████| 776.8MB 22kB/s \n",
      "\u001B[?25hRequirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.7/dist-packages (from flair) (4.41.1)\n",
      "Collecting segtok>=1.5.7\n",
      "  Downloading https://files.pythonhosted.org/packages/41/08/582dab5f4b1d5ca23bc6927b4bb977c8ff7f3a87a3b98844ef833e2f5623/segtok-1.5.10.tar.gz\n",
      "Collecting ftfy\n",
      "\u001B[?25l  Downloading https://files.pythonhosted.org/packages/af/da/d215a091986e5f01b80f5145cff6f22e2dc57c6b048aab2e882a07018473/ftfy-6.0.3.tar.gz (64kB)\n",
      "\u001B[K     |████████████████████████████████| 71kB 10.9MB/s \n",
      "\u001B[?25hCollecting huggingface-hub\n",
      "  Downloading https://files.pythonhosted.org/packages/32/a1/7c5261396da23ec364e296a4fb8a1cd6a5a2ff457215c6447038f18c0309/huggingface_hub-0.0.9-py3-none-any.whl\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from flair) (0.22.2.post1)\n",
      "Requirement already satisfied: numpy<1.20.0 in /usr/local/lib/python3.7/dist-packages (from flair) (1.19.5)\n",
      "Collecting mpld3==0.3\n",
      "\u001B[?25l  Downloading https://files.pythonhosted.org/packages/91/95/a52d3a83d0a29ba0d6898f6727e9858fe7a43f6c2ce81a5fe7e05f0f4912/mpld3-0.3.tar.gz (788kB)\n",
      "\u001B[K     |████████████████████████████████| 798kB 58.6MB/s \n",
      "\u001B[?25hCollecting transformers>=4.0.0\n",
      "\u001B[?25l  Downloading https://files.pythonhosted.org/packages/d5/43/cfe4ee779bbd6a678ac6a97c5a5cdeb03c35f9eaebbb9720b036680f9a2d/transformers-4.6.1-py3-none-any.whl (2.2MB)\n",
      "\u001B[K     |████████████████████████████████| 2.3MB 39.1MB/s \n",
      "\u001B[?25hRequirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from flair) (4.2.6)\n",
      "Collecting sqlitedict>=1.6.0\n",
      "  Downloading https://files.pythonhosted.org/packages/5c/2d/b1d99e9ad157dd7de9cd0d36a8a5876b13b55e4b75f7498bc96035fb4e96/sqlitedict-1.7.0.tar.gz\n",
      "Collecting langdetect\n",
      "\u001B[?25l  Downloading https://files.pythonhosted.org/packages/0e/72/a3add0e4eec4eb9e2569554f7c70f4a3c27712f40e3284d483e88094cc0e/langdetect-1.0.9.tar.gz (981kB)\n",
      "\u001B[K     |████████████████████████████████| 983kB 53.2MB/s \n",
      "\u001B[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from flair) (2.8.1)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated>=1.2.4->flair) (1.12.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from bpemb>=0.3.2->flair) (2.23.0)\n",
      "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim<=3.8.3,>=3.4.0->flair) (1.15.0)\n",
      "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim<=3.8.3,>=3.4.0->flair) (5.0.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim<=3.8.3,>=3.4.0->flair) (1.4.1)\n",
      "Collecting overrides<4.0.0,>=3.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/ff/b1/10f69c00947518e6676bbd43e739733048de64b8dd998e9c2d5a71f44c5d/overrides-3.1.0.tar.gz\n",
      "Collecting importlib-metadata<4.0.0,>=3.7.0\n",
      "  Downloading https://files.pythonhosted.org/packages/52/d0/bdb31463f2d9ca111e39b268518e9baa3542ef73ca449b711a7b4da69764/importlib_metadata-3.10.1-py3-none-any.whl\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair) (2.5.1)\n",
      "Requirement already satisfied: pymongo in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair) (3.11.4)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair) (0.16.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (1.3.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (2.4.7)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from gdown==3.12.2->flair) (3.0.12)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch<=1.7.1,>=1.5.0->flair) (3.7.4.3)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy->flair) (0.2.5)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->flair) (1.0.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->flair) (20.9)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "\u001B[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
      "\u001B[K     |████████████████████████████████| 3.3MB 29.7MB/s \n",
      "\u001B[?25hCollecting sacremoses\n",
      "\u001B[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
      "\u001B[K     |████████████████████████████████| 901kB 46.2MB/s \n",
      "\u001B[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair) (2020.12.5)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair) (3.0.4)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair) (3.4.1)\n",
      "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx->hyperopt>=0.1.1->flair) (4.4.2)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4.0.0->flair) (7.1.2)\n",
      "Building wheels for collected packages: gdown\n",
      "  Building wheel for gdown (PEP 517) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for gdown: filename=gdown-3.12.2-cp37-none-any.whl size=9693 sha256=fdb44c70584bb9183c0945ab5036ade7b0ab8352264ba52140bb2a263e1848c7\n",
      "  Stored in directory: /root/.cache/pip/wheels/81/d0/d7/d9983facc6f2775411803e0e2d30ebf98efbf2fc6e57701e09\n",
      "Successfully built gdown\n",
      "Building wheels for collected packages: segtok, ftfy, mpld3, sqlitedict, langdetect, overrides\n",
      "  Building wheel for segtok (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for segtok: filename=segtok-1.5.10-cp37-none-any.whl size=25019 sha256=1779251e6cb7b7740be9b28e67788ee0c6348bfb3347de5fc238c06b5ab426c6\n",
      "  Stored in directory: /root/.cache/pip/wheels/b4/39/f6/9ca1c5cabde964d728023b5751c3a206a5c8cc40252321fb6b\n",
      "  Building wheel for ftfy (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for ftfy: filename=ftfy-6.0.3-cp37-none-any.whl size=41916 sha256=506ec94b2d81b73b6a10bc2f885178114f14e96e99664d1cc7eec294dd5de372\n",
      "  Stored in directory: /root/.cache/pip/wheels/99/2c/e6/109c8a28fef7a443f67ba58df21fe1d0067ac3322e75e6b0b7\n",
      "  Building wheel for mpld3 (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for mpld3: filename=mpld3-0.3-cp37-none-any.whl size=116679 sha256=0eb985f71c328561f6753d5403b71516b4646cd7f7bbd3f6ad7c7ebf0cfea7e8\n",
      "  Stored in directory: /root/.cache/pip/wheels/c0/47/fb/8a64f89aecfe0059830479308ad42d62e898a3e3cefdf6ba28\n",
      "  Building wheel for sqlitedict (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for sqlitedict: filename=sqlitedict-1.7.0-cp37-none-any.whl size=14376 sha256=29ebe7987af1874154520b38da4f4c00bae1ef815524992873db58539a86dec6\n",
      "  Stored in directory: /root/.cache/pip/wheels/cf/c6/4f/2c64a43f041415eb8b8740bd80e15e92f0d46c5e464d8e4b9b\n",
      "  Building wheel for langdetect (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-cp37-none-any.whl size=993223 sha256=447b0cb7a843f84f52b374d0b6f56abb3b4a522ebfe628ff59eb0e46ad6b44ea\n",
      "  Stored in directory: /root/.cache/pip/wheels/7e/18/13/038c34057808931c7ddc6c92d3aa015cf1a498df5a70268996\n",
      "  Building wheel for overrides (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for overrides: filename=overrides-3.1.0-cp37-none-any.whl size=10174 sha256=20ac3719d05a10caea67f6a4f8443ae3776fb035cab5a09a212f6e453d935f59\n",
      "  Stored in directory: /root/.cache/pip/wheels/5c/24/13/6ef8600e6f147c95e595f1289a86a3cc82ed65df57582c65a9\n",
      "Successfully built segtok ftfy mpld3 sqlitedict langdetect overrides\n",
      "\u001B[31mERROR: torchvision 0.9.1+cu101 has requirement torch==1.8.1, but you'll have torch 1.7.1 which is incompatible.\u001B[0m\n",
      "\u001B[31mERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.7.1 which is incompatible.\u001B[0m\n",
      "\u001B[31mERROR: konoha 4.6.5 has requirement requests<3.0.0,>=2.25.1, but you'll have requests 2.23.0 which is incompatible.\u001B[0m\n",
      "\u001B[31mERROR: transformers 4.6.1 has requirement huggingface-hub==0.0.8, but you'll have huggingface-hub 0.0.9 which is incompatible.\u001B[0m\n",
      "Installing collected packages: sentencepiece, deprecated, bpemb, overrides, importlib-metadata, konoha, janome, gdown, torch, segtok, ftfy, huggingface-hub, mpld3, tokenizers, sacremoses, transformers, sqlitedict, langdetect, flair\n",
      "  Found existing installation: importlib-metadata 4.0.1\n",
      "    Uninstalling importlib-metadata-4.0.1:\n",
      "      Successfully uninstalled importlib-metadata-4.0.1\n",
      "  Found existing installation: gdown 3.6.4\n",
      "    Uninstalling gdown-3.6.4:\n",
      "      Successfully uninstalled gdown-3.6.4\n",
      "  Found existing installation: torch 1.8.1+cu101\n",
      "    Uninstalling torch-1.8.1+cu101:\n",
      "      Successfully uninstalled torch-1.8.1+cu101\n",
      "Successfully installed bpemb-0.3.3 deprecated-1.2.12 flair-0.8.0.post1 ftfy-6.0.3 gdown-3.12.2 huggingface-hub-0.0.9 importlib-metadata-3.10.1 janome-0.4.1 konoha-4.6.5 langdetect-1.0.9 mpld3-0.3 overrides-3.1.0 sacremoses-0.0.45 segtok-1.5.10 sentencepiece-0.1.95 sqlitedict-1.7.0 tokenizers-0.10.3 torch-1.7.1 transformers-4.6.1\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "importlib_metadata"
        ]
       }
      }
     },
     "metadata": {
      "tags": []
     }
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UKoPUsQGgaEE",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "2119972a-5493-4abf-9503-d3e7ff0218f9"
   },
   "source": [
    "from flair.data import Corpus\n",
    "from flair.data_fetcher import NLPTaskDataFetcher\n",
    "\n",
    "\n",
    "# your training file name\n",
    "data_folder = os.getcwd() \n",
    "\n",
    "train_file = \"training_data.csv\"\n",
    "\n",
    "# your training file name\n",
    "test_file = \"test_data.csv\"\n",
    "\n",
    "# when we wrote the data files, each row was either empty to signal the end\n",
    "# of a sentence to Flair, or the line contained a token, a white space and a label.\n",
    "# In the next line, we assign, that the token is the \"text\", and that the label is \n",
    "# \"ner\" label\n",
    "columns =  {0: 'text', 1: 'ner'}\n",
    "\n",
    "## Now load our csv into flair corpus\n",
    "corpus = NLPTaskDataFetcher.load_column_corpus(data_folder,column_format=columns,\n",
    "                                               train_file=train_file,\n",
    "                                               test_file=test_file)\n",
    "print(corpus)"
   ],
   "execution_count": 41,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "2021-06-02 07:35:32,289 Reading data from /content/gdrive/My Drive/flair\n",
      "2021-06-02 07:35:32,290 Train: /content/gdrive/My Drive/flair/training_data.csv\n",
      "2021-06-02 07:35:32,296 Dev: None\n",
      "2021-06-02 07:35:32,297 Test: /content/gdrive/My Drive/flair/test_data.csv\n"
     ],
     "name": "stdout"
    },
    {
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: DeprecationWarning: Call to deprecated function (or staticmethod) load_column_corpus. (Use 'flair.datasets' instead.) -- Deprecated since version 0.4.1.\n"
     ],
     "name": "stderr"
    },
    {
     "output_type": "stream",
     "text": [
      "Corpus: 6150 train + 683 dev + 3120 test sentences\n"
     ],
     "name": "stdout"
    }
   ]
  }
 ]
}