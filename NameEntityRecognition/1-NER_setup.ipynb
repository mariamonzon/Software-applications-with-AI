{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "ner_1_2021.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GgDUExqz5ioj"
   },
   "source": [
    "Set up Google Colab Drive"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rblxfoSULPza"
   },
   "source": [
    "#!pip install google-colab"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3chPOl3o5MrV",
    "outputId": "abb6a1e8-22d1-474c-8667-b58d04955f0d"
   },
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VXJqiu7P6T2p"
   },
   "source": [
    "Deep learning needs the computational power of GPUs, therefore, we run this notebook on Google Colab with GPU support.<br>\n",
    "Check RAM of GPU.<br>\n",
    "For this end, we need to install [GPUtil](https://pypi.org/project/GPUtil/)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gumhOAG16vYh",
    "outputId": "19672812-570f-442d-b835-7cfda4b28599"
   },
   "source": [
    "pip install GPUtil"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: GPUtil in /usr/local/lib/python3.7/dist-packages (1.4.0)\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WkK4Lfch6MGo",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 393
    },
    "outputId": "8bdd166c-1ec7-4676-95d5-111e28f98e78"
   },
   "source": [
    "import psutil\n",
    "import humanize\n",
    "import os\n",
    "import GPUtil as GPU\n",
    "GPUs = GPU.getGPUs()\n",
    "gpu = GPUs[0]\n",
    "\n",
    "def printm():\n",
    "  process = psutil.Process(os.getpid())\n",
    "  print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" I Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
    "  print('GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB'.format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
    "\n",
    "printm()"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "ignored",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-7-9a928fb499fd>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mGPUtil\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mGPU\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m \u001B[0mGPUs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mGPU\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgetGPUs\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      6\u001B[0m \u001B[0mgpu\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mGPUs\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.7/dist-packages/GPUtil/GPUtil.py\u001B[0m in \u001B[0;36mgetGPUs\u001B[0;34m()\u001B[0m\n\u001B[1;32m    100\u001B[0m             \u001B[0;31m# print(vals[i])\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    101\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mi\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 102\u001B[0;31m                 \u001B[0mdeviceIds\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvals\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    103\u001B[0m             \u001B[0;32melif\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mi\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    104\u001B[0m                 \u001B[0muuid\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mvals\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: invalid literal for int() with base 10: \"NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\""
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M3-quF-P-XXH"
   },
   "source": [
    "Change directory to \"/flair\""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "BBTFZUFp_fY2",
    "outputId": "a2d7f31b-c68a-4eca-cffb-ce1b56e0ad07"
   },
   "source": [
    "os.getcwd()"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content'"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 9
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lRq9VJeA_sSH"
   },
   "source": [
    "So, we are currently in \"/content\""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_1VO3JH8ADUt",
    "outputId": "56daf295-eb3d-4faa-98d8-8fddfa84fe65"
   },
   "source": [
    "for directory in os.walk( os.getcwd() ):\n",
    "\n",
    "  print( directory )"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "('/content', ['.config', 'gdrive', 'sample_data'], [])\n",
      "('/content/.config', ['logs', 'configurations'], ['gce', '.last_survey_prompt.yaml', '.last_update_check.json', 'active_config', '.last_opt_in_prompt.yaml', 'config_sentinel'])\n",
      "('/content/.config/logs', ['2021.05.06'], [])\n",
      "('/content/.config/logs/2021.05.06', [], ['13.43.39.026092.log', '13.44.01.543195.log', '13.43.04.692209.log', '13.43.23.909017.log', '13.43.44.620859.log', '13.44.00.991142.log'])\n",
      "('/content/.config/configurations', [], ['config_default'])\n",
      "('/content/gdrive', ['.file-revisions-by-id', '.shortcut-targets-by-id', 'MyDrive', '.Trash'], [])\n",
      "('/content/gdrive/.file-revisions-by-id', [], [])\n",
      "('/content/gdrive/.shortcut-targets-by-id', [], [])\n",
      "('/content/gdrive/MyDrive', ['Cumpleaños Latzi', 'Colab Notebooks', 'Docs', 'flair'], ['ThesisFinalShort (2).pptx', 'ThesisFinalShort (1).pptx', 'ThesisFinalShort.pptx', 'ThesisFinalUni (1).pptx', 'ThesisFinalUni.pptx', 'AbstractISMRM (2).docx', 'AbstractISMRM (1).docx', 'AbstractISMRM.docx'])\n",
      "('/content/gdrive/MyDrive/Cumpleaños Latzi', [], ['Lock Forms.gform', 'Felicidades Latzi.gsite'])\n",
      "('/content/gdrive/MyDrive/Colab Notebooks', [], ['Copia de resumee_ner.ipynb', 'ner_1_2021.ipynb'])\n",
      "('/content/gdrive/MyDrive/Docs', [], ['Application letter (1).docx', 'Application letter.docx', 'applicattion letter (1).docx', 'applicattion letter.docx', 'Argudio testua (1).doc', 'Argudio testua.doc', 'English connectors.doc', 'Essay transport.docx', 'Gizakia vs natura.docx', 'I liked very much the book East 43rd Street.docx', 'Idiomas ingles (1).docx', 'Idiomas ingles.docx', 'Idioms.docx', 'Phrasal verbs.docx'])\n",
      "('/content/gdrive/MyDrive/flair', [], ['Entity Recognition in Resumes.json'])\n",
      "('/content/gdrive/.Trash', [], [])\n",
      "('/content/sample_data', [], ['anscombe.json', 'README.md', 'california_housing_test.csv', 'mnist_train_small.csv', 'mnist_test.csv', 'california_housing_train.csv'])\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hhCgEKfqA8mU"
   },
   "source": [
    "We see in the table above, that there exists a folder \"/content/gdrive/MyDrive/flair\" into which we have loaded our data.<br>\n",
    "We now change the working directory to this directory."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LefqzTtQA229"
   },
   "source": [
    "os.chdir( \"/content/gdrive/MyDrive/flair\" ) "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "r88--TlKBW9r",
    "outputId": "c6302ccf-d507-4afd-8fa3-9101030eae7a"
   },
   "source": [
    "os.getcwd()"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/gdrive/MyDrive/flair'"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 12
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SV3KJ2RoLPzk"
   },
   "source": [
    "Using the code from above, we change the current working directory to the directory, where we loaded our data to."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1KpNb75WC1RS",
    "outputId": "44e6e8d3-db44-4628-e4ab-c759fc88f19b"
   },
   "source": [
    "os.listdir( os.getcwd() )"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Entity Recognition in Resumes.json']"
      ]
     },
     "metadata": {
      "tags": []
     },
     "execution_count": 13
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PvSVxTV4ERCb"
   },
   "source": [
    "Install Flair"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s6UgvIiOET3h",
    "outputId": "d7ca10c0-3601-43b1-ac3c-38666764eda4"
   },
   "source": [
    "pip install flair"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Collecting flair\n",
      "\u001B[?25l  Downloading https://files.pythonhosted.org/packages/f0/3a/1b46a0220d6176b22bcb9336619d1731301bc2c75fa926a9ef953e6e4d58/flair-0.8.0.post1-py3-none-any.whl (284kB)\n",
      "\u001B[K     |████████████████████████████████| 286kB 8.5MB/s \n",
      "\u001B[?25hCollecting bpemb>=0.3.2\n",
      "  Downloading https://files.pythonhosted.org/packages/f2/6f/9191b85109772636a8f8accb122900c34db26c091d2793218aa94954524c/bpemb-0.3.3-py3-none-any.whl\n",
      "Collecting segtok>=1.5.7\n",
      "  Downloading https://files.pythonhosted.org/packages/41/08/582dab5f4b1d5ca23bc6927b4bb977c8ff7f3a87a3b98844ef833e2f5623/segtok-1.5.10.tar.gz\n",
      "Requirement already satisfied: tqdm>=4.26.0 in /usr/local/lib/python3.7/dist-packages (from flair) (4.41.1)\n",
      "Collecting transformers>=4.0.0\n",
      "\u001B[?25l  Downloading https://files.pythonhosted.org/packages/d5/43/cfe4ee779bbd6a678ac6a97c5a5cdeb03c35f9eaebbb9720b036680f9a2d/transformers-4.6.1-py3-none-any.whl (2.2MB)\n",
      "\u001B[K     |████████████████████████████████| 2.3MB 30.3MB/s \n",
      "\u001B[?25hRequirement already satisfied: numpy<1.20.0 in /usr/local/lib/python3.7/dist-packages (from flair) (1.19.5)\n",
      "Collecting deprecated>=1.2.4\n",
      "  Downloading https://files.pythonhosted.org/packages/fb/73/994edfcba74443146c84b91921fcc269374354118d4f452fb0c54c1cbb12/Deprecated-1.2.12-py2.py3-none-any.whl\n",
      "Collecting sqlitedict>=1.6.0\n",
      "  Downloading https://files.pythonhosted.org/packages/5c/2d/b1d99e9ad157dd7de9cd0d36a8a5876b13b55e4b75f7498bc96035fb4e96/sqlitedict-1.7.0.tar.gz\n",
      "Collecting huggingface-hub\n",
      "  Downloading https://files.pythonhosted.org/packages/32/a1/7c5261396da23ec364e296a4fb8a1cd6a5a2ff457215c6447038f18c0309/huggingface_hub-0.0.9-py3-none-any.whl\n",
      "Collecting janome\n",
      "\u001B[?25l  Downloading https://files.pythonhosted.org/packages/a8/63/98858cbead27df7536c7e300c169da0999e9704d02220dc6700b804eeff0/Janome-0.4.1-py2.py3-none-any.whl (19.7MB)\n",
      "\u001B[K     |████████████████████████████████| 19.7MB 1.3MB/s \n",
      "\u001B[?25hRequirement already satisfied: hyperopt>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from flair) (0.1.2)\n",
      "Collecting mpld3==0.3\n",
      "\u001B[?25l  Downloading https://files.pythonhosted.org/packages/91/95/a52d3a83d0a29ba0d6898f6727e9858fe7a43f6c2ce81a5fe7e05f0f4912/mpld3-0.3.tar.gz (788kB)\n",
      "\u001B[K     |████████████████████████████████| 798kB 38.9MB/s \n",
      "\u001B[?25hRequirement already satisfied: gensim<=3.8.3,>=3.4.0 in /usr/local/lib/python3.7/dist-packages (from flair) (3.6.0)\n",
      "Collecting ftfy\n",
      "\u001B[?25l  Downloading https://files.pythonhosted.org/packages/af/da/d215a091986e5f01b80f5145cff6f22e2dc57c6b048aab2e882a07018473/ftfy-6.0.3.tar.gz (64kB)\n",
      "\u001B[K     |████████████████████████████████| 71kB 8.4MB/s \n",
      "\u001B[?25hRequirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from flair) (4.2.6)\n",
      "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from flair) (0.8.9)\n",
      "Collecting konoha<5.0.0,>=4.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/71/70/48a0bd55f79c328504fe6fe7ae8ff651f77a2aadbb1911701385d9bb5ca3/konoha-4.6.5-py3-none-any.whl\n",
      "Collecting langdetect\n",
      "\u001B[?25l  Downloading https://files.pythonhosted.org/packages/0e/72/a3add0e4eec4eb9e2569554f7c70f4a3c27712f40e3284d483e88094cc0e/langdetect-1.0.9.tar.gz (981kB)\n",
      "\u001B[K     |████████████████████████████████| 983kB 33.3MB/s \n",
      "\u001B[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from flair) (2.8.1)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from flair) (0.22.2.post1)\n",
      "Requirement already satisfied: matplotlib>=2.2.3 in /usr/local/lib/python3.7/dist-packages (from flair) (3.2.2)\n",
      "Collecting torch<=1.7.1,>=1.5.0\n",
      "\u001B[?25l  Downloading https://files.pythonhosted.org/packages/90/5d/095ddddc91c8a769a68c791c019c5793f9c4456a688ddd235d6670924ecb/torch-1.7.1-cp37-cp37m-manylinux1_x86_64.whl (776.8MB)\n",
      "\u001B[K     |████████████████████████████████| 776.8MB 12kB/s \n",
      "\u001B[?25hCollecting gdown==3.12.2\n",
      "  Downloading https://files.pythonhosted.org/packages/50/21/92c3cfe56f5c0647145c4b0083d0733dd4890a057eb100a8eeddf949ffe9/gdown-3.12.2.tar.gz\n",
      "  Installing build dependencies ... \u001B[?25l\u001B[?25hdone\n",
      "  Getting requirements to build wheel ... \u001B[?25l\u001B[?25hdone\n",
      "    Preparing wheel metadata ... \u001B[?25l\u001B[?25hdone\n",
      "Collecting sentencepiece==0.1.95\n",
      "\u001B[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
      "\u001B[K     |████████████████████████████████| 1.2MB 33.8MB/s \n",
      "\u001B[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from flair) (2019.12.20)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from bpemb>=0.3.2->flair) (2.23.0)\n",
      "Collecting sacremoses\n",
      "\u001B[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
      "\u001B[K     |████████████████████████████████| 901kB 47.9MB/s \n",
      "\u001B[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->flair) (3.0.12)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->flair) (4.0.1)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "\u001B[?25l  Downloading https://files.pythonhosted.org/packages/d4/e2/df3543e8ffdab68f5acc73f613de9c2b155ac47f162e725dcac87c521c11/tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3MB)\n",
      "\u001B[K     |████████████████████████████████| 3.3MB 27.7MB/s \n",
      "\u001B[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers>=4.0.0->flair) (20.9)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.7/dist-packages (from deprecated>=1.2.4->flair) (1.12.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub->flair) (3.7.4.3)\n",
      "Requirement already satisfied: pymongo in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair) (3.11.4)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair) (1.4.1)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair) (2.5.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair) (1.15.0)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from hyperopt>=0.1.1->flair) (0.16.0)\n",
      "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim<=3.8.3,>=3.4.0->flair) (5.0.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from ftfy->flair) (0.2.5)\n",
      "Collecting overrides<4.0.0,>=3.0.0\n",
      "  Downloading https://files.pythonhosted.org/packages/ff/b1/10f69c00947518e6676bbd43e739733048de64b8dd998e9c2d5a71f44c5d/overrides-3.1.0.tar.gz\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->flair) (1.0.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (0.10.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2.3->flair) (2.4.7)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->bpemb>=0.3.2->flair) (3.0.4)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers>=4.0.0->flair) (7.1.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers>=4.0.0->flair) (3.4.1)\n",
      "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx->hyperopt>=0.1.1->flair) (4.4.2)\n",
      "Building wheels for collected packages: gdown\n",
      "  Building wheel for gdown (PEP 517) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for gdown: filename=gdown-3.12.2-cp37-none-any.whl size=9693 sha256=ee1efe3e0a5eb5370c9118417355e1e0f4d5af755bb3864348e77874bdfbd58e\n",
      "  Stored in directory: /root/.cache/pip/wheels/81/d0/d7/d9983facc6f2775411803e0e2d30ebf98efbf2fc6e57701e09\n",
      "Successfully built gdown\n",
      "Building wheels for collected packages: segtok, sqlitedict, mpld3, ftfy, langdetect, overrides\n",
      "  Building wheel for segtok (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for segtok: filename=segtok-1.5.10-cp37-none-any.whl size=25019 sha256=284fb15cfdf3102e5794211d69707df8cf85c7ba3d73095cd56714d6d21ae67f\n",
      "  Stored in directory: /root/.cache/pip/wheels/b4/39/f6/9ca1c5cabde964d728023b5751c3a206a5c8cc40252321fb6b\n",
      "  Building wheel for sqlitedict (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for sqlitedict: filename=sqlitedict-1.7.0-cp37-none-any.whl size=14376 sha256=a6b5ab57d36c430d8b59a71470d273a9bd29d0eea1ad51f7cbf58ce92d24928c\n",
      "  Stored in directory: /root/.cache/pip/wheels/cf/c6/4f/2c64a43f041415eb8b8740bd80e15e92f0d46c5e464d8e4b9b\n",
      "  Building wheel for mpld3 (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for mpld3: filename=mpld3-0.3-cp37-none-any.whl size=116679 sha256=0c1c3464acd2ded1b055ec1260fa572b5db7a634acb4df2edef6ef88c6408c6d\n",
      "  Stored in directory: /root/.cache/pip/wheels/c0/47/fb/8a64f89aecfe0059830479308ad42d62e898a3e3cefdf6ba28\n",
      "  Building wheel for ftfy (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for ftfy: filename=ftfy-6.0.3-cp37-none-any.whl size=41916 sha256=c580fe2460d654beb58e331902bda0c5cee2cc899bb4a2edf0e45f122a2a6e92\n",
      "  Stored in directory: /root/.cache/pip/wheels/99/2c/e6/109c8a28fef7a443f67ba58df21fe1d0067ac3322e75e6b0b7\n",
      "  Building wheel for langdetect (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-cp37-none-any.whl size=993223 sha256=eee0e32f788f616676627fe140e8803972474d6b85f27dbc0b246908919fc8fb\n",
      "  Stored in directory: /root/.cache/pip/wheels/7e/18/13/038c34057808931c7ddc6c92d3aa015cf1a498df5a70268996\n",
      "  Building wheel for overrides (setup.py) ... \u001B[?25l\u001B[?25hdone\n",
      "  Created wheel for overrides: filename=overrides-3.1.0-cp37-none-any.whl size=10174 sha256=fb31f2e05a39b2bef7314c93965909d94e0d7d95078ae73f0f52ab3487fc1892\n",
      "  Stored in directory: /root/.cache/pip/wheels/5c/24/13/6ef8600e6f147c95e595f1289a86a3cc82ed65df57582c65a9\n",
      "Successfully built segtok sqlitedict mpld3 ftfy langdetect overrides\n",
      "\u001B[31mERROR: torchvision 0.9.1+cu101 has requirement torch==1.8.1, but you'll have torch 1.7.1 which is incompatible.\u001B[0m\n",
      "\u001B[31mERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.7.1 which is incompatible.\u001B[0m\n",
      "\u001B[31mERROR: transformers 4.6.1 has requirement huggingface-hub==0.0.8, but you'll have huggingface-hub 0.0.9 which is incompatible.\u001B[0m\n",
      "\u001B[31mERROR: konoha 4.6.5 has requirement importlib-metadata<4.0.0,>=3.7.0, but you'll have importlib-metadata 4.0.1 which is incompatible.\u001B[0m\n",
      "\u001B[31mERROR: konoha 4.6.5 has requirement requests<3.0.0,>=2.25.1, but you'll have requests 2.23.0 which is incompatible.\u001B[0m\n",
      "Installing collected packages: sentencepiece, bpemb, segtok, sacremoses, tokenizers, huggingface-hub, transformers, deprecated, sqlitedict, janome, mpld3, ftfy, overrides, konoha, langdetect, torch, gdown, flair\n",
      "  Found existing installation: torch 1.8.1+cu101\n",
      "    Uninstalling torch-1.8.1+cu101:\n",
      "      Successfully uninstalled torch-1.8.1+cu101\n",
      "  Found existing installation: gdown 3.6.4\n",
      "    Uninstalling gdown-3.6.4:\n",
      "      Successfully uninstalled gdown-3.6.4\n",
      "Successfully installed bpemb-0.3.3 deprecated-1.2.12 flair-0.8.0.post1 ftfy-6.0.3 gdown-3.12.2 huggingface-hub-0.0.9 janome-0.4.1 konoha-4.6.5 langdetect-1.0.9 mpld3-0.3 overrides-3.1.0 sacremoses-0.0.45 segtok-1.5.10 sentencepiece-0.1.95 sqlitedict-1.7.0 tokenizers-0.10.3 torch-1.7.1 transformers-4.6.1\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OKbUtk3gUIUH"
   },
   "source": [
    "Import the Data Set"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0hwSlDozTvTu"
   },
   "source": [
    "path_to_data = os.getcwd() + '/Entity Recognition in Resumes.json'"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TC736kedLPzm"
   },
   "source": [
    "__Task 1__: Complete the following code: we want to read the data from said .json-file into the list __imported_data__. Also, print the first line, so that we get an idea, what the data look like. Also, print how many resumees we got."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "StK5oNS8Uo7G",
    "outputId": "f8e00f78-7ff4-4de5-b552-5117827120a7"
   },
   "source": [
    "myfile = open( path_to_data, \"r\", encoding = \"utf-8\" )\n",
    "\n",
    "imported_data = []\n",
    "\n",
    "for datum in myfile:\n",
    "    \n",
    "  # process data as list\n",
    "  imported_data.append(datum)\n",
    "\n",
    "myfile.close()\n",
    "\n",
    "# print first line\n",
    "print(\"First Line:\\t\", imported_data[0])\n",
    "\n",
    "# print how many resumees were read in\n",
    "print(\"Total Number of Resumes:\\t\", len(imported_data))\n"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "First Line:\t {\"content\": \"Afreen Jamadar\\nActive member of IIIT Committee in Third year\\n\\nSangli, Maharashtra - Email me on Indeed: indeed.com/r/Afreen-Jamadar/8baf379b705e37c6\\n\\nI wish to use my knowledge, skills and conceptual understanding to create excellent team\\nenvironments and work consistently achieving organization objectives believes in taking initiative\\nand work to excellence in my work.\\n\\nWORK EXPERIENCE\\n\\nActive member of IIIT Committee in Third year\\n\\nCisco Networking -  Kanpur, Uttar Pradesh\\n\\norganized by Techkriti IIT Kanpur and Azure Skynet.\\nPERSONALLITY TRAITS:\\n• Quick learning ability\\n• hard working\\n\\nEDUCATION\\n\\nPG-DAC\\n\\nCDAC ACTS\\n\\n2017\\n\\nBachelor of Engg in Information Technology\\n\\nShivaji University Kolhapur -  Kolhapur, Maharashtra\\n\\n2016\\n\\nSKILLS\\n\\nDatabase (Less than 1 year), HTML (Less than 1 year), Linux. (Less than 1 year), MICROSOFT\\nACCESS (Less than 1 year), MICROSOFT WINDOWS (Less than 1 year)\\n\\nADDITIONAL INFORMATION\\n\\nTECHNICAL SKILLS:\\n\\n• Programming Languages: C, C++, Java, .net, php.\\n• Web Designing: HTML, XML\\n• Operating Systems: Windows […] Windows Server 2003, Linux.\\n• Database: MS Access, MS SQL Server 2008, Oracle 10g, MySql.\\n\\nhttps://www.indeed.com/r/Afreen-Jamadar/8baf379b705e37c6?isid=rex-download&ikw=download-top&co=IN\",\"annotation\":[{\"label\":[\"Email Address\"],\"points\":[{\"start\":1155,\"end\":1198,\"text\":\"indeed.com/r/Afreen-Jamadar/8baf379b705e37c6\"}]},{\"label\":[\"Links\"],\"points\":[{\"start\":1143,\"end\":1239,\"text\":\"https://www.indeed.com/r/Afreen-Jamadar/8baf379b705e37c6?isid=rex-download&ikw=download-top&co=IN\"}]},{\"label\":[\"Skills\"],\"points\":[{\"start\":743,\"end\":1140,\"text\":\"Database (Less than 1 year), HTML (Less than 1 year), Linux. (Less than 1 year), MICROSOFT\\nACCESS (Less than 1 year), MICROSOFT WINDOWS (Less than 1 year)\\n\\nADDITIONAL INFORMATION\\n\\nTECHNICAL SKILLS:\\n\\n• Programming Languages: C, C++, Java, .net, php.\\n• Web Designing: HTML, XML\\n• Operating Systems: Windows […] Windows Server 2003, Linux.\\n• Database: MS Access, MS SQL Server 2008, Oracle 10g, MySql.\"}]},{\"label\":[\"Graduation Year\"],\"points\":[{\"start\":729,\"end\":732,\"text\":\"2016\"}]},{\"label\":[\"College Name\"],\"points\":[{\"start\":675,\"end\":702,\"text\":\"Shivaji University Kolhapur \"}]},{\"label\":[\"Degree\"],\"points\":[{\"start\":631,\"end\":672,\"text\":\"Bachelor of Engg in Information Technology\"}]},{\"label\":[\"Graduation Year\"],\"points\":[{\"start\":625,\"end\":629,\"text\":\"2017\\n\"}]},{\"label\":[\"College Name\"],\"points\":[{\"start\":614,\"end\":622,\"text\":\"CDAC ACTS\"}]},{\"label\":[\"Degree\"],\"points\":[{\"start\":606,\"end\":611,\"text\":\"PG-DAC\"}]},{\"label\":[\"Companies worked at\"],\"points\":[{\"start\":438,\"end\":453,\"text\":\"Cisco Networking\"}]},{\"label\":[\"Email Address\"],\"points\":[{\"start\":104,\"end\":147,\"text\":\"indeed.com/r/Afreen-Jamadar/8baf379b705e37c6\"}]},{\"label\":[\"Location\"],\"points\":[{\"start\":62,\"end\":67,\"text\":\"Sangli\"}]},{\"label\":[\"Name\"],\"points\":[{\"start\":0,\"end\":13,\"text\":\"Afreen Jamadar\"}]}],\"extras\":null,\"metadata\":{\"first_done_at\":1527844872000,\"last_updated_at\":1537724086000,\"sec_taken\":0,\"last_updated_by\":\"BIQNZm4INNfvByMqkaVwVt6OZTv2\",\"status\":\"done\",\"evaluation\":\"CORRECT\"}}\n",
      "\n",
      "Total Number of Resumes:\t 701\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I1ZSnIfOWLP_"
   },
   "source": [
    "Map the dataset to json"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "We0edT5BWRTv"
   },
   "source": [
    "import json"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vI-fguXrWTvo"
   },
   "source": [
    "mapped_data = [ json.loads( datum ) for datum in imported_data  ]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2NZHylfPLPzn"
   },
   "source": [
    "Now, the data are stored in __mapped_data__. __mapped_data__ is a list, and its entries are dictionaries.<br>\n",
    "__Task 2__: choose an entry of __mapped_data__, iterate over its keys, print the key and its corresponding value."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "exSHAxQFYBbp",
    "outputId": "5cea701f-f337-4970-d537-5dc91cc28284"
   },
   "source": [
    "#choose an entry of mapped_data, iterate over its keys, print the key and its corresponding value\n",
    "entry = 10\n",
    "for key in mapped_data[entry].keys():\n",
    "    print(\"KEY:\", key, \"\\t VALUE:\", mapped_data[entry][key])\n"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "KEY: content \t VALUE: Bhawana Daf\n",
      "Pune, Maharashtra - Email me on Indeed: indeed.com/r/Bhawana-Daf/d9ddb6a54519d583\n",
      "\n",
      "Seeking a career in preschool where I can utilize my teaching background strongly ,in nurturing\n",
      "young minds ,demonstrating quality teaching skills.\n",
      "\n",
      "Willing to relocate to: Viman Nagar, Maharashtra - Vadgaonsheri - Kharadi, Maharashtra\n",
      "\n",
      "WORK EXPERIENCE\n",
      "\n",
      "Preschool Teacher\n",
      "\n",
      "Pune, Maharashtra -\n",
      "\n",
      "2015 to 2018\n",
      "\n",
      "Class teacher\n",
      "\n",
      "Data Entry and Discrepancy Management\n",
      "\n",
      "Oracle Clinical\n",
      "\n",
      "4.6)\n",
      "• Knowledge of clinical trial data like Demographic Data, Adverse Events (AE), Serious Adverse\n",
      "Events (SAE), Laboratory Data (Lab Data) etc.\n",
      "\n",
      "EDUCATION\n",
      "\n",
      "B. Sc. in Biology\n",
      "\n",
      "Government Science College -  Pandhurna, Madhya Pradesh\n",
      "\n",
      "H.S.C\n",
      "\n",
      "R.D.H.S. School -  Chhindwara, Madhya Pradesh\n",
      "\n",
      "Post Graduate Diploma in Clinical Data Management\n",
      "\n",
      "R.D.H.S. School -  Chhindwara, Madhya Pradesh\n",
      "\n",
      "SKILLS\n",
      "\n",
      "Teaching (3 years)\n",
      "\n",
      "ADDITIONAL INFORMATION\n",
      "\n",
      "SKILLS:\n",
      "• Perform all functions related to Data Entry like First Pass and Second Pass Data Entry, review\n",
      "of Data Entry, and database update. Well versed in reading hand written patient documents.\n",
      "\n",
      "https://www.indeed.com/r/Bhawana-Daf/d9ddb6a54519d583?isid=rex-download&ikw=download-top&co=IN\n",
      "\n",
      "\n",
      "• Review, analyze, and validate clinical trial data to ensure consistency, integrity and accuracy\n",
      "based on project specific guidelines.\n",
      "\n",
      "• Maintain clinical trial data accuracy through review of case report forms for completeness and\n",
      "consistency.\n",
      "\n",
      "• Query data inconsistencies and revise case report forms in compliance with standard operating\n",
      "procedures, client guidelines and regulatory agency guidelines.\n",
      "KEY: annotation \t VALUE: [{'label': ['Skills'], 'points': [{'start': 868, 'end': 875, 'text': 'Teaching'}]}, {'label': ['College Name'], 'points': [{'start': 813, 'end': 827, 'text': 'R.D.H.S. School'}]}, {'label': ['Degree'], 'points': [{'start': 762, 'end': 810, 'text': 'Post Graduate Diploma in Clinical Data Management'}]}, {'label': ['College Name'], 'points': [{'start': 715, 'end': 729, 'text': 'R.D.H.S. School'}]}, {'label': ['Degree'], 'points': [{'start': 708, 'end': 712, 'text': 'H.S.C'}]}, {'label': ['College Name'], 'points': [{'start': 651, 'end': 676, 'text': 'Government Science College'}]}, {'label': ['Degree'], 'points': [{'start': 632, 'end': 648, 'text': 'B. Sc. in Biology'}]}, {'label': ['Companies worked at'], 'points': [{'start': 457, 'end': 471, 'text': 'Oracle Clinical'}]}, {'label': ['Designation'], 'points': [{'start': 403, 'end': 415, 'text': 'Class teacher'}]}, {'label': ['Location'], 'points': [{'start': 368, 'end': 371, 'text': 'Pune'}]}, {'label': ['Designation'], 'points': [{'start': 349, 'end': 365, 'text': 'Preschool Teacher'}]}, {'label': ['Email Address'], 'points': [{'start': 52, 'end': 93, 'text': 'indeed.com/r/Bhawana-Daf/d9ddb6a54519d583\\n'}]}, {'label': ['Location'], 'points': [{'start': 12, 'end': 15, 'text': 'Pune'}]}, {'label': ['Name'], 'points': [{'start': 0, 'end': 10, 'text': 'Bhawana Daf'}]}]\n",
      "KEY: extras \t VALUE: None\n",
      "KEY: metadata \t VALUE: {'first_done_at': 1527762973000, 'last_updated_at': 1527762973000, 'sec_taken': 125, 'last_updated_by': 'jI67aE5hwwdh6l16bcfFVnpyREd2', 'status': 'done', 'evaluation': 'NONE'}\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cbVeAOnCPPnR"
   },
   "source": [
    ""
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8gp7URulapTm"
   },
   "source": [
    "__Question 1__: What are the keys, and what information do these keys store?<br>\n",
    "\n",
    "THE keys are content, annotation extras and metadata which store data about a candidate resumee, valoration of skilss anotaiton, extra information and data of the file creation respectively\n",
    "\n",
    "__Task 3__: for each entry of __annotations__: iterate over the keys of this entry and print the key and the corresponding value."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K9WcXpq4bTee",
    "outputId": "c075672f-bf2b-48b1-9ba1-4e3386171c84"
   },
   "source": [
    "# TODO for each entry of annotations: iterate over the keys of this entry and print the key and the corresponding value\n",
    "\n",
    "entry = 15\n",
    "for a in mapped_data[entry][\"annotation\"][0].keys():\n",
    "    print(\"KEY:\", a, \"\\t VALUE:\", mapped_data[entry][\"annotation\"][0][a])\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "KEY: label \t VALUE: ['Skills']\n",
      "KEY: points \t VALUE: [{'start': 5893, 'end': 6042, 'text': 'problem solving (Less than 1 year), project lifecycle (Less than 1 year), project manager (Less\\nthan 1 year), technical assistance. (Less than 1 year)'}]\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6i-ZPKiyfyrt"
   },
   "source": [
    "__Question 2__: What did you learn about the __annotations__? Give an example.<br>\n",
    "\n",
    "The annotations are in a form of list-dict nested format and usually store skill/experience scores\n",
    "\n",
    "__Task 4__: Complete the following code to map the __mapped_data__ to a format, from which Spacy can learn. Print the first converted resumee for inspection. Choose one resumee. For this resumee, print all the data in __entities__."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mzmu0XSOhybH",
    "outputId": "f0be8a6f-9bfa-4e57-cfc2-44bfce88e793"
   },
   "source": [
    "## data conversion method\n",
    "def convert_data(data):\n",
    "    \"\"\"\n",
    "    Creates NER training data in Spacy format from JSON dataset\n",
    "    Outputs the Spacy training data which can be used for Spacy training.\n",
    "    \"\"\"\n",
    "    text = data['content']\n",
    "    entities = []\n",
    "    if data['annotation'] is not None:\n",
    "        for annotation in data['annotation']:\n",
    "            # only a single point in text annotation.\n",
    "            point = annotation['points'][0]\n",
    "            labels = annotation['label']\n",
    "            # handle both list of labels or a single label.\n",
    "            if not isinstance(labels, list):\n",
    "                labels = [labels]\n",
    "            for label in labels:\n",
    "                # dataturks indices are both inclusive [start, end] but spacy is not [start, end)\n",
    "                entities.append((point['start'], point['end'] + 1, label))\n",
    "    return (text, {\"entities\": entities})\n",
    "   \n",
    "## Using a loop or list comprehension, convert each resume in mapped_data using the convert function above, \n",
    "## storing the result\n",
    "converted_resumes = [convert_data(resume) for resume in mapped_data]\n",
    "## print the number of resumes in converted resumes \n",
    "print(\"Converted resumes:\\t\", len(converted_resumes))"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Converted resumes:\t 701\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2jO6WUBLiaD0",
    "outputId": "f8f59218-efd4-45d6-b17a-38fd005d6d9a"
   },
   "source": [
    "# TODO print the first resumee for inspection\n",
    "for element in converted_resumes[0]: \n",
    "  print(element)\n",
    "  print(20*\"-\")"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Afreen Jamadar\n",
      "Active member of IIIT Committee in Third year\n",
      "\n",
      "Sangli, Maharashtra - Email me on Indeed: indeed.com/r/Afreen-Jamadar/8baf379b705e37c6\n",
      "\n",
      "I wish to use my knowledge, skills and conceptual understanding to create excellent team\n",
      "environments and work consistently achieving organization objectives believes in taking initiative\n",
      "and work to excellence in my work.\n",
      "\n",
      "WORK EXPERIENCE\n",
      "\n",
      "Active member of IIIT Committee in Third year\n",
      "\n",
      "Cisco Networking -  Kanpur, Uttar Pradesh\n",
      "\n",
      "organized by Techkriti IIT Kanpur and Azure Skynet.\n",
      "PERSONALLITY TRAITS:\n",
      "• Quick learning ability\n",
      "• hard working\n",
      "\n",
      "EDUCATION\n",
      "\n",
      "PG-DAC\n",
      "\n",
      "CDAC ACTS\n",
      "\n",
      "2017\n",
      "\n",
      "Bachelor of Engg in Information Technology\n",
      "\n",
      "Shivaji University Kolhapur -  Kolhapur, Maharashtra\n",
      "\n",
      "2016\n",
      "\n",
      "SKILLS\n",
      "\n",
      "Database (Less than 1 year), HTML (Less than 1 year), Linux. (Less than 1 year), MICROSOFT\n",
      "ACCESS (Less than 1 year), MICROSOFT WINDOWS (Less than 1 year)\n",
      "\n",
      "ADDITIONAL INFORMATION\n",
      "\n",
      "TECHNICAL SKILLS:\n",
      "\n",
      "• Programming Languages: C, C++, Java, .net, php.\n",
      "• Web Designing: HTML, XML\n",
      "• Operating Systems: Windows […] Windows Server 2003, Linux.\n",
      "• Database: MS Access, MS SQL Server 2008, Oracle 10g, MySql.\n",
      "\n",
      "https://www.indeed.com/r/Afreen-Jamadar/8baf379b705e37c6?isid=rex-download&ikw=download-top&co=IN\n",
      "--------------------\n",
      "{'entities': [(1155, 1199, 'Email Address'), (1143, 1240, 'Links'), (743, 1141, 'Skills'), (729, 733, 'Graduation Year'), (675, 703, 'College Name'), (631, 673, 'Degree'), (625, 630, 'Graduation Year'), (614, 623, 'College Name'), (606, 612, 'Degree'), (438, 454, 'Companies worked at'), (104, 148, 'Email Address'), (62, 68, 'Location'), (0, 14, 'Name')]}\n",
      "--------------------\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6FDv_kfXi4MF",
    "outputId": "af62c4d2-46ad-4c2f-c9de-6691e96ed971"
   },
   "source": [
    "# TODO choose one resumee. For this resumee, print all the data in entities. Use the dumps function from json.\n",
    "resumee = 10\n",
    "print(\"Entities:\\n\", json.dumps(converted_resumes[resumee][1][\"entities\"]))\n"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Entities:\n",
      " [[868, 876, \"Skills\"], [813, 828, \"College Name\"], [762, 811, \"Degree\"], [715, 730, \"College Name\"], [708, 713, \"Degree\"], [651, 677, \"College Name\"], [632, 649, \"Degree\"], [457, 472, \"Companies worked at\"], [403, 416, \"Designation\"], [368, 372, \"Location\"], [349, 366, \"Designation\"], [52, 94, \"Email Address\"], [12, 16, \"Location\"], [0, 11, \"Name\"]]\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lXx23PxAjPBj"
   },
   "source": [
    "__Task 5__: Explain the printout from above.<br>\n",
    "\n",
    "__Task 6__: Since some resumees have no annotation, we want to filter these out. You can recognize these resumees by them having no entries in __entities__. Pick one of the remaining resumees, iterate over the items in __entitities__, print for each item the label and the corresponding text from the resumee."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8KjAdqTckp41"
   },
   "source": [
    "#filter out the resumees whose entities have no entries.\n",
    "converted_complete_resumees = [resume for resume in converted_resumes if len(resume[1][\"entities\"]) > 0]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Al0x9iyBk-qs",
    "outputId": "52336ffa-d6ac-4ae3-f70c-42aba54329d3"
   },
   "source": [
    "print( len( converted_complete_resumees ) )"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "690\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qys_4RJ6nCCj",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "fe0318ca-2544-465a-d7b4-cd558fa63e6a"
   },
   "source": [
    "# TODO Now, pick one resumee from converted_complete_resumees, and iterate over the items in \n",
    "# entities. Print for each item the label and the corresponding text from the resumee.\n",
    "resumee = 15\n",
    "for item in converted_complete_resumees[resumee][1][\"entities\"]: \n",
    "  print(item)\n",
    "#print(\"Filetered Resumee:\", json.dumps(converted_complete_resumees[resumee][1]))\n"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "(5893, 6043, 'Skills')\n",
      "(5879, 5883, 'Graduation Year')\n",
      "(5838, 5859, 'College Name')\n",
      "(3730, 3740, 'Companies worked at')\n",
      "(3319, 3329, 'Companies worked at')\n",
      "(3237, 3247, 'Companies worked at')\n",
      "(3147, 3157, 'Companies worked at')\n",
      "(3099, 3109, 'Companies worked at')\n",
      "(2059, 2069, 'Companies worked at')\n",
      "(1977, 1987, 'Companies worked at')\n",
      "(1887, 1897, 'Companies worked at')\n",
      "(1837, 1847, 'Companies worked at')\n",
      "(1772, 1782, 'Companies worked at')\n",
      "(1753, 1771, 'Designation')\n",
      "(729, 739, 'Companies worked at')\n",
      "(90, 143, 'Location')\n",
      "(37, 46, 'Location')\n",
      "(0, 14, 'Name')\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QJCOSsZJtDnA"
   },
   "source": [
    "__Question 3__: Are the labels unique? If the labels are not unique, does this make the data ambigious?<br>\n",
    "\n",
    "The labels are not unique and taht may cause ambigueties \n",
    "\n",
    "__Task 7__: Collect the names of all entities in __converted_complete_resumee__ dataset, and store these names in __all_labels__. Then iterate over the contents of __all_labels__, and store each name that does not appear in __unique_labels__ in __unique_labels__, so that __unique_labels__ contains each name that appears in __all_labels__, but only once."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "i8N5z6OutflX",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "519144d4-1e0d-48f6-fdcc-fe42a5cbea3a"
   },
   "source": [
    "# TODO Collect the names of all entities in converted_complete_resumee dataset, and store these names \n",
    "# in all_labels. Then iterate over the contents of all_labels, and store each name that does not appear \n",
    "# in unique_labels in unique_labels, so that unique_labels contains each name that appears in \n",
    "# all_labels, but only once.\n",
    "\n",
    "all_labels = [entry[-1] for resume in converted_complete_resumees for entry in resume[1][\"entities\"]]\n",
    "unique_labels = []\n",
    "\n",
    "for label in all_labels: \n",
    "  if label not in unique_labels: \n",
    "    unique_labels.append(label)\n",
    "\n",
    "\n",
    "for item in unique_labels:\n",
    "  print( item )"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Email Address\n",
      "Links\n",
      "Skills\n",
      "Graduation Year\n",
      "College Name\n",
      "Degree\n",
      "Companies worked at\n",
      "Location\n",
      "Name\n",
      "Designation\n",
      "projects\n",
      "Years of Experience\n",
      "Can Relocate to\n",
      "UNKNOWN\n",
      "Rewards and Achievements\n",
      "Address\n",
      "University\n",
      "Relocate to\n",
      "Certifications\n",
      "state\n",
      "links\n",
      "College\n",
      "training\n",
      "des\n",
      "abc\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yM9qBbtGtHfb"
   },
   "source": [
    "# Nueva sección"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fxJ_ZC6Zv0q1"
   },
   "source": [
    "__Task 8__: Now choose three labels, on which you want to train your named entity recognition algorithm. You want to have for each label at least 300 documents."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3KgZMpQTwiIu",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d72f661e-8110-488f-d432-2ce446412fc5"
   },
   "source": [
    "## TODO store entity label names for the entities you want to work with in an array \n",
    "chosen_entity_label = [ \"Skills\", \"College Name\", \"Email Address\" ]\n",
    "## for each chosen entity label, count how many documents have a labeled entity for that label, and how many labeled entities total there are \n",
    "## for that entity\n",
    "for chosen in chosen_entity_label:\n",
    "    found_docs_with_entity = 0\n",
    "    entity_count = 0\n",
    "    for resume in converted_complete_resumees:\n",
    "        entity_list = resume[1][\"entities\"]\n",
    "        _,_,labels = zip(*entity_list)\n",
    "        if chosen in labels:\n",
    "            found_docs_with_entity+=1\n",
    "            entity_count+=len([l for l in labels if l == chosen])\n",
    "    print(\"Docs with {}: {}\".format(chosen,found_docs_with_entity))\n",
    "    print(\"Total count of {}: {}\".format(chosen,entity_count))"
   ],
   "execution_count": null,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "Docs with Skills: 536\n",
      "Total count of Skills: 2152\n",
      "Docs with College Name: 497\n",
      "Total count of College Name: 1160\n",
      "Docs with Email Address: 640\n",
      "Total count of Email Address: 967\n"
     ],
     "name": "stdout"
    }
   ]
  }
 ]
}